{"title":"A Replication of Karlan and List (2007)","markdown":{"yaml":{"title":"A Replication of Karlan and List (2007)","author":"Sumedh Hambarde","date":"today","callout-appearance":"minimal"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nDean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).\n\nThis project seeks to replicate their results.\n\n### Background\n\nFundraisers often rely on rules of thumb rather than scientific evidence to guide their strategies.  While the economics of charity has been studied from the \"supply\" side (e.g., the impact of tax deductions), there are still \"critical gaps\" in understanding the \"demand\" side (i.e., how and why donors choose to give).    \n\nOne common fundraising tool is the matching grant, where a donor pledges to match the contributions of others.  Fundraisers often believe that higher matching ratios (e.g., 2:1 or 3:1) are more effective in attracting donations, but there is limited research to support this. \n\n### Purpose of the Study (Research Question)\n\nKarlan and List (2007) aimed to address this gap by examining the impact of matching grants on charitable giving through a large-scale natural field experiment.  Specifically, the study investigates whether and to what extent \"price\" (in the form of matching grant ratios) matters in charitable fundraising.    \n\n### Background of the charity\n\nThe experiment was conducted in collaboration with a liberal non-profit organization in the United States that focuses on social and policy issues related to civil liberties.  The organization is a 501(c)3 charity, meaning donations are tax-deductible.  The organization regularly solicits donations from its prior donors through direct mail campaigns.    \n\nThe organization's membership has the following characteristics:\n\n1. Approximately 70% male\n2. Approximately 60% over 65 years old\n3. Approximately 80% with a college education\n4. Political leaning: 85% voted for Gore in the 2000 presidential election \n\n### Experimental Design\n\nThe researchers conducted a natural field experiment using a direct mail solicitation to over 50,000 past donors of the organization.  Individuals were randomly assigned to either a treatment group that received a matching grant offer or a control group that did not.  The treatment group was further randomized to receive different matching grant ratios. \n\n### What Was the Treatment Received by the Groups\n\nAll participants received a four-page fundraising letter.    \n\nControl Group: Received a standard letter with no mention of a matching grant.    \nTreatment Groups: Received a letter that included an additional paragraph announcing the matching grant offer.  The matching grant had different price ratios:\n1:1 match: For every dollar donated, the organization receives two dollars.    \n2:1 match: For every dollar donated, the organization receives three dollars.    \n3:1 match: For every dollar donated, the organization receives four dollars.\n\n\n![](additionalParagraph.png)\n\n\n## Data\n\n### Description\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n| Variable             | Description                                                         |\n|----------------------|---------------------------------------------------------------------|\n| `treatment`          | Treatment                                                           |\n| `control`            | Control                                                             |\n| `ratio`              | Match ratio                                                         |\n| `ratio2`             | 2:1 match ratio                                                     |\n| `ratio3`             | 3:1 match ratio                                                     |\n| `size`               | Match threshold                                                     |\n| `size25`             | \\$25,000 match threshold                                            |\n| `size50`             | \\$50,000 match threshold                                            |\n| `size100`            | \\$100,000 match threshold                                           |\n| `sizeno`             | Unstated match threshold                                            |\n| `ask`                | Suggested donation amount                                           |\n| `askd1`              | Suggested donation was highest previous contribution                |\n| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |\n| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |\n| `ask1`               | Highest previous contribution (for suggestion)                      |\n| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |\n| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |\n| `amount`             | Dollars given                                                       |\n| `gave`               | Gave anything                                                       |\n| `amountchange`       | Change in amount given                                              |\n| `hpa`                | Highest previous contribution                                       |\n| `ltmedmra`           | Small prior donor: last gift was less than median \\$35              |\n| `freq`               | Number of prior donations                                           |\n| `years`              | Number of years since initial donation                              |\n| `year5`              | At least 5 years since initial donation                             |\n| `mrm2`               | Number of months since last donation                                |\n| `dormant`            | Already donated in 2005                                             |\n| `female`             | Female                                                              |\n| `couple`             | Couple                                                              |\n| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |\n| `nonlit`             | Nonlitigation                                                       |\n| `cases`              | Court cases from state in 2004-5 in which organization was involved |\n| `statecnt`           | Percent of sample from state                                        |\n| `stateresponse`      | Proportion of sample from the state who gave                        |\n| `stateresponset`     | Proportion of treated sample from the state who gave                |\n| `stateresponsec`     | Proportion of control sample from the state who gave                |\n| `stateresponsetminc` | stateresponset - stateresponsec                                     |\n| `perbush`            | State vote share for Bush                                           |\n| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |\n| `red0`               | Red state                                                           |\n| `blue0`              | Blue state                                                          |\n| `redcty`             | Red county                                                          |\n| `bluecty`            | Blue county                                                         |\n| `pwhite`             | Proportion white within zip code                                    |\n| `pblack`             | Proportion black within zip code                                    |\n| `page18_39`          | Proportion age 18-39 within zip code                                |\n| `ave_hh_sz`          | Average household size within zip code                              |\n| `median_hhincome`    | Median household income within zip code                             |\n| `powner`             | Proportion house owner within zip code                              |\n| `psch_atlstba`       | Proportion who finished college within zip code                     |\n| `pop_propurban`      | Proportion of population urban within zip code                      |\n\n::::\n\n```{python}\nimport pandas as pd\n\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\n#print(data.head())\n\nprint(data.info())\n\nprint(data.describe())\n```\n\n\n### Balance Tests: Checking the success of randomization\n\nIn any experiment that uses random assignment, it's crucial to verify that the randomization process was successful in creating comparable groups. This ensures that any differences I observe in donation behavior are likely due to the matching grant offers and not simply due to pre-existing differences between the groups. To do this, I conducted balance tests on several pre-treatment variables.\n\n#### Methodology\n\nTo assess the balance between the treatment and control groups, I performed two types of statistical tests for each pre-treatment variable:\n\n- T-tests: I used independent samples t-tests to compare the means of each variable between the treatment and control groups. The null hypothesis for each t-test was that there is no difference in the average value of the variable between the two groups.\n\n- Linear Regressions: I also ran a series of simple linear regressions. In each regression, the pre-treatment variable was the dependent variable, and the treatment assignment (coded as 0 for control and 1 for treatment) was the independent variable. This allowed me to estimate the average difference in the pre-treatment variable between the treatment and control groups.\n\nIt's important to note that, as the professor emphasized, the t-test and the regression approach are two ways of examining the same thing and should give me very similar results.\n\n#### Results\n\nI conducted these balance tests on the following pre-treatment variables:\n\n1. Months since last donation\n2. Number of prior donations\n3. Female (an indicator variable for gender)\n4. Couple (an indicator variable for whether the individual is part of a couple)\n\nThe code for this, as well as the results from the balance tests are as follows: \n\n```{python code-fold=\"true\"}\nimport pandas as pd\nfrom scipy import stats\nimport statsmodels.formula.api as sm\nimport numpy as np\n\n# Load your data\ndata = pd.read_stata('dataverse_files/AER merged.dta')  # Or pd.read_csv(), etc.\n\n#print(data[['treatment', 'control']].head(2))\ndef test_balance(var_value, var_name):\n\n    print('\\n')\n    variable_to_test = var_value\n\n    # T-test\n    control_group = data[data['control'] == 1][variable_to_test]\n    treatment_group = data[data['treatment'] == 1][variable_to_test]\n    #print(treatment_group.head(2))\n    #print(control_group.head(2))\n\n    # Calculate means\n    mean_control = np.mean(control_group)\n    mean_treatment = np.mean(treatment_group)\n\n    # Calculate variances\n    variance_control = np.var(control_group, ddof=1)  # ddof=1 for sample variance\n    variance_treatment = np.var(treatment_group, ddof=1)\n\n    # Calculate sample sizes\n    n_control = len(control_group)\n    n_treatment = len(treatment_group)\n\n    # Calculate pooled variance\n    pooled_variance = ((n_treatment - 1) * variance_treatment + (n_control - 1) * variance_control) / (n_treatment + n_control - 2)\n\n    # Calculate pooled standard error\n    pooled_standard_error = np.sqrt(pooled_variance * (1 / n_treatment + 1 / n_control))\n\n    # Calculate t-statistic\n    t_statistic_manual = (mean_treatment - mean_control) / pooled_standard_error\n    print(f\"t-statistic for {var_name}: t = {t_statistic_manual:.3f}\")\n\n    df = n_treatment + n_control - 2\n\n    p_value_manual_two_tailed = stats.t.sf(abs(t_statistic_manual), df) * 2\n\n    print(f\"Two-Tailed p-value: {p_value_manual_two_tailed:.3f}\")\n\n\n\n    # Linear Regression\n    formula = f'{variable_to_test} ~ treatment'\n    model = sm.ols(formula, data=data).fit()\n    p_value_reg = model.pvalues['treatment']\n    print(f\"Regression for {var_name}: p = {p_value_reg:.3f}\")\n\n    # Interpretation\n    if p_value_manual_two_tailed < 0.05:\n        print(f\"For {var_name}, there is a statistically significant difference between treatment and control groups.\")\n    else:\n        print(f\"For {var_name}, there is no statistically significant difference between treatment and control groups.\")\n\n\ntest_balance('mrm2', 'Months since last donation')\ntest_balance('freq', 'Number of Prior Donations')\n#test_balance('perbush', 'State vote share for Bush')\n#test_balance('cases', 'Court cases from 2004-05 in which organization was involved')\ntest_balance('female', 'Female')\ntest_balance('couple', 'Couple')\n# Repeat for other variables\n```\n\n#### Overall Assessment of Randomization\n\nBased on these balance tests, I conclude that the randomization process in the Karlan and List (2007) experiment appears to have been largely successful. For all four pre-treatment variables I examined, I did not find statistically significant differences between the treatment and control groups at the 0.05 significance level.\n\nWhile some minor differences are expected due to chance, especially in large samples, the general lack of statistical significance suggests that the treatment and control groups are indeed comparable. This strengthens the internal validity of the study and supports the authors' ability to attribute any observed differences in donation behavior to the matching grant offers.\n\n#### Importance of Balance Tests\n\nBalance tests are a critical component of any experimental study, and Karlan and List (2007) included them for good reason. These tests serve several important purposes:\n\n- Establishing Internal Validity: They provide evidence that the treatment, rather than pre-existing differences, is the likely cause of any observed effects.\n- Supporting Causal Inference: They justify making causal claims about the impact of the treatment.\n- Ensuring Rigor: They demonstrate the rigor of the experimental design and increase confidence in the study's findings.\n- Promoting Transparency: They allow other researchers to assess the quality of the experiment and attempt to replicate the results.\n\n## Experimental Results\n\n### Charitable Contribution Made\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. \n\n#### Barplot for proportion of people who donated\n```{python code-fold=\"true\"}\nimport pandas as pd\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\nimport plotly.graph_objects as go\n\ndef plot_donation_by_group(data):\n    # 2. Calculate donation proportions\n    proportion_gave_treatment = data[data['treatment'] == 1]['gave'].mean()\n    proportion_gave_control = data[data['control'] == 1]['gave'].mean()\n\n    # 3. Prepare data for plotting\n    plot_data = pd.DataFrame({\n        'group': ['Treatment', 'Control'],\n        'proportion_gave': [proportion_gave_treatment, proportion_gave_control]\n    })\n\n    # 4. Create the bar plot\n    fig = go.Figure(data=[\n        go.Bar(x=plot_data['group'], y=plot_data['proportion_gave'])\n    ])\n\n    fig.add_trace(go.Scatter(\n        x=plot_data['group'],\n        y=plot_data['proportion_gave'],\n        text=[f\"{100 * p:.2f}%\" for p in plot_data['proportion_gave']],  # Format proportions\n        mode='text',\n        textposition='top center'\n    ))\n    # 5. Customize the plot\n    fig.update_layout(\n        title='Proportion of Donors by Group',\n        xaxis_title='Group',\n        yaxis_title='Proportion Donated',\n        yaxis_range=[0, max(plot_data['proportion_gave']) * 1.1]  # Extend y-axis slightly\n    )\n\n    # 6. Display the plot\n    fig.show()\n\n\nplot_donation_by_group(data)\n```\n\nThe results of my analysis show that the treatment group, which received the matching grant offer, had a higher donation rate (2.2%) compared to the control group (1.79%). This suggests that the matching grant may have had a positive impact on the likelihood of individuals making a donation. However, it's important to keep in mind that this is just a preliminary observation, and further statistical testing is needed to determine if this difference is statistically significant.\n\n#### Binary donation outcome: t-test and it's interpretation:\n```{python}\nimport pandas as pd\nfrom scipy import stats  # For t-test\n\ndef perform_t_test_on_donation(data):\n    \"\"\"\n    Performs an independent samples t-test on the binary donation outcome.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'treatment', 'control', and 'gave' columns.\n\n    Returns:\n        tuple: (t-statistic, p-value)\n    \"\"\"\n    # --- T-test ---\n    treatment_gave = data[data['treatment'] == 1]['gave']\n    control_gave = data[data['control'] == 1]['gave']\n    t_statistic, p_value_ttest = stats.ttest_ind(treatment_gave, control_gave)\n\n    return t_statistic, p_value_ttest\n\n\nif __name__ == '__main__':\n    # --- Load Your Actual Data Here ---\n    data = pd.read_stata('dataverse_files/AER merged.dta')  # Replace with your data loading\n\n    t_statistic, p_value = perform_t_test_on_donation(data)\n\n    print(\"--- T-test Results ---\")\n    print(f\"T-statistic: {t_statistic:.3f}\")\n    print(f\"P-value: {p_value:.3f}\")\n\n    # You would then proceed to interpret these results\n```\n\n#### Interpretation of the t-test:\n\nThe t-test results indicate a statistically significant difference in donation behavior between the group that received the matching grant offer (the treatment group) and the group that did not (the control group). The p-value of 0.002 is substantially lower than the typical significance level of 0.05, meaning that the observed difference is unlikely to have occurred by chance alone. In other words, there's strong evidence to suggest that the matching grant offer had a real effect on whether people decided to donate.\n\nThe positive t-statistic (3.101) suggests that the treatment group had a higher donation rate. This finding aligns with the idea that individuals respond to price-like incentives, even in charitable giving. The matching grant effectively reduces the 'cost' of giving; for every dollar a person donates, the charity receives more. This increased 'bang for your buck' appears to motivate people to be more generous.\n\nFrom a behavioral perspective, this result highlights that while altruism and a desire to support a cause are important motivators for charitable giving, financial incentives can also play a significant role. People seem to weigh the perceived value of their contribution, and matching grants can significantly shift that calculation. This underscores the potential effectiveness of matching grant campaigns as a fundraising strategy.\n\n#### Probit regression: Results and interpretation\n\n```{python}\nimport pandas as pd\nimport statsmodels.api as sm  # For Probit regression\n\ndef perform_probit_regression(data):\n    \"\"\"\n    Performs a probit regression to model the probability of donation.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'treatment', 'control', and 'gave' columns.\n\n    Returns:\n        sm.ProbitResults: The results of the probit regression.\n    \"\"\"\n\n    # --- Verify that 'treatment' and 'control' are mutually exclusive ---\n    # --- Create treatment dummy variable ---\n    data['treatment_dummy'] = data['treatment']  # Assuming 'treatment' is 1/0\n\n    # --- Probit Regression ---\n    formula = 'gave ~ treatment_dummy'\n    probit_model = sm.Probit(data['gave'], sm.add_constant(data['treatment_dummy']))\n    probit_results = probit_model.fit()\n\n    return probit_results\n\n\nif __name__ == '__main__':\n    data = pd.read_stata('dataverse_files/AER merged.dta')\n    probit_results = perform_probit_regression(data)\n    \n    # --- Print Results Summary ---\n    print(probit_results.summary())\n\n    # --- Access specific results ---\n    print(\"\\nProbit Regression Results:\")\n    print(f\"Coefficient on treatment: {probit_results.params['treatment_dummy']:.4f}\")\n    print(f\"P-value on treatment: {probit_results.pvalues['treatment_dummy']:.4f}\")\n```\n\n#### Interpretation of the results:\n\nThe probit regression results indicate a statistically significant positive effect of the matching grant offer on the probability of making a charitable donation. The coefficient on the treatment_dummy variable is 0.0868, and it has a p-value of 0.002, which is well below the conventional significance level of 0.05. This suggests that being offered a matching grant significantly increased the likelihood that an individual would donate.\n\nTo understand the magnitude of this effect, we need to consider that the coefficients in a probit regression represent the change in the z-score of the standard normal distribution associated with a one-unit change in the predictor variable (in this case, going from the control group to the treatment group). While the coefficient itself isn't a direct measure of the change in probability, its positive and statistically significant value clearly demonstrates that the matching offer had a positive impact on the propensity to give.\n\nIn behavioral terms, this finding reinforces the idea that the 'price' mechanism introduced by the matching grant influences donation decisions. The offer of having their donation matched appears to have provided an additional incentive, nudging individuals who received the treatment letter towards donating at a higher rate than those in the control group who did not receive such an offer. This aligns with economic theories suggesting that individuals respond to incentives, even in the realm of altruistic behavior.\n\n### Differences between Match Rates\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n_todo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the \"figures suggest\" comment the authors make on page 8?_\n\n#### Pairwise t-tests: Results\n```{python}\nimport pandas as pd\nfrom scipy import stats  # For t-test\n\ndef compare_donation_rates_by_ratio(data):\n    \"\"\"\n    Compares donation rates across different match ratios using t-tests.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'ratio' and 'gave' columns.\n\n    Returns:\n        dict: A dictionary containing the t-test results for each comparison.\n    \"\"\"\n\n    results = {}\n\n    # 1:1 vs. 2:1\n    gave_1 = data[data['ratio'] == 1]['gave']\n    gave_2 = data[data['ratio'] == 2]['gave']\n    t_stat, p_value = stats.ttest_ind(gave_1, gave_2)\n    results['1:1 vs. 2:1'] = {'t_statistic': t_stat, 'p_value': p_value}\n\n    # 1:1 vs. 3:1\n    gave_1 = data[data['ratio'] == 1]['gave']\n    gave_3 = data[data['ratio'] == 3]['gave']\n    t_stat, p_value = stats.ttest_ind(gave_1, gave_3)\n    results['1:1 vs. 3:1'] = {'t_statistic': t_stat, 'p_value': p_value}\n\n    # 2:1 vs. 3:1\n    gave_2 = data[data['ratio'] == 2]['gave']\n    gave_3 = data[data['ratio'] == 3]['gave']\n    t_stat, p_value = stats.ttest_ind(gave_2, gave_3)\n    results['2:1 vs. 3:1'] = {'t_statistic': t_stat, 'p_value': p_value}\n\n    return results\n\n\nif __name__ == '__main__':\n    # --- Load Your Actual Data Here ---\n    data = pd.read_stata('dataverse_files/AER merged.dta')  # Replace with your data loading\n\n    results = compare_donation_rates_by_ratio(data)\n\n    # --- Print Results ---\n    print(\"T-test Results: Donation Rates by Match Ratio\")\n    for comparison, values in results.items():\n        print(f\"\\nComparison: {comparison}\")\n        print(f\"  T-statistic: {values['t_statistic']:.3f}\")\n        print(f\"  P-value: {values['p_value']:.3f}\")\n```\n\n#### Pairwise t-tests: Interpretation:\n**Lack of Statistical Significance:**\n\n- For all three comparisons, the p-values are considerably greater than the common significance level of 0.05.\n- This means that we fail to reject the null hypothesis in each case. The null hypothesis states that there is no difference in the average donation rates between the two groups being compared.\n- In simpler terms, the data does not provide enough evidence to conclude that changing the match ratio (from 1:1 to 2:1, from 1:1 to 3:1, or from 2:1 to 3:1) has a statistically significant impact on the proportion of people who donate.\n- These results match the conclusions made in the original research publication.\n\n**Behavioral Implications:**\n\n- These findings might suggest that, within the range of match ratios tested, the specific generosity of the match offer doesn't strongly influence the likelihood of donating.\n- It's possible that the presence of any match is more important than the size of the match. People might be motivated by the simple fact that their donation will be multiplied, without being very sensitive to the exact multiplication factor.\n- Another possibility is that the differences in match ratios tested were not large enough to elicit a statistically detectable change in behavior.\n\n\n\n```{python}\nimport statsmodels.formula.api as sm  # For OLS regression\n\nimport pandas as pd\ndata = pd.read_stata('dataverse_files/AER merged.dta')\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\n\n# --- Regression ---\nformula = 'gave ~ ratio1 + ratio2 + ratio3'\nmodel = sm.ols(formula, data=data).fit()\n\n# --- Print Results Summary ---\nprint(model.summary())\n\n# --- Interpretation ---\nprint(\"\\nRegression Results Interpretation:\")\nprint(\"----------------------------\")\n\nprint(f\"Coefficient for ratio1: {model.params['ratio1']:.4f}\")\nprint(f\"P-value for ratio1: {model.pvalues['ratio1']:.4f}\")\n\nprint(f\"Coefficient for ratio2: {model.params['ratio2']:.4f}\")\nprint(f\"P-value for ratio2: {model.pvalues['ratio2']:.4f}\")\n\nprint(f\"Coefficient for ratio3: {model.params['ratio3']:.4f}\")\nprint(f\"P-value for ratio3: {model.pvalues['ratio3']:.4f}\")\n\n\n```\n\nInterpretation of the OLS Regression Results\n\nIn this OLS regression, we modeled the binary outcome of whether a donation was made (gave) as a function of dummy variables representing the different match ratios (ratio1 for 1:1, ratio2 for 2:1, and ratio3 for 3:1). The \"Control\" group (no match) serves as the implicit reference category.\n\nHere's a breakdown of the key findings:\n\nIntercept (Coefficient: 0.0179, P-value: 0.000): The intercept represents the predicted probability of donation for the reference group, which is the \"Control\" group (where ratio1, ratio2, and ratio3 are all 0). A coefficient of 0.0179 suggests that in the absence of any matching offer, the predicted donation rate is approximately 1.79%. This is statistically significant (p < 0.001).\n\nCoefficient for ratio1 (0.0029, P-value: 0.097): This coefficient estimates the additional change in the probability of donation when the match ratio is 1:1, compared to the Control group. The positive coefficient (0.0029) suggests that the 1:1 match is associated with a 0.29 percentage point increase in the predicted donation rate (1.79% + 0.29% = 2.08%). However, the p-value of 0.097 is greater than the conventional significance level of 0.05. Therefore, we do not have statistically significant evidence that the 1:1 matching offer significantly increased donations compared to the control group in this OLS model.\n\nCoefficient for ratio2 (0.0048, P-value: 0.006): This coefficient estimates the additional change in the probability of donation when the match ratio is 2:1, compared to the Control group. The positive and statistically significant coefficient (0.0048, p = 0.006) indicates that the 2:1 matching offer led to a significant increase of 0.48 percentage points in the predicted donation rate compared to the Control group (1.79% + 0.48% = 2.27%).\n\nCoefficient for ratio3 (0.0049, P-value: 0.005): This coefficient estimates the additional change in the probability of donation when the match ratio is 3:1, compared to the Control group. The positive and statistically significant coefficient (0.0049, p = 0.005) shows that the 3:1 matching offer resulted in a significant increase of 0.49 percentage points in the predicted donation rate compared to the Control group (1.79% + 0.49% = 2.28%).\n\nOverall Interpretation:\n\nThe OLS regression results suggest that while a 1:1 matching offer did not significantly increase donation rates compared to no offer, both the 2:1 and 3:1 matching offers had a statistically significant positive impact on the likelihood of donation. The magnitudes of the effects for the 2:1 and 3:1 matches are quite similar, suggesting that increasing the match ratio beyond 2:1 might not lead to a substantial further increase in the proportion of donors.\n\nThese findings are broadly consistent with the idea that matching grants incentivize charitable giving. The stronger effects observed for the higher match ratios (2:1 and 3:1) provide some evidence that the \"price\" of giving matters, with a more generous match leading to a greater increase in donation propensity compared to no match. However, the lack of a significant effect for the 1:1 match hints at a possible threshold effect or a weaker influence at that particular ratio in this model.\n\n\n```{python}\nimport pandas as pd\nimport statsmodels.formula.api as sm\n\ndef calculate_direct_response_rate_differences(data):\n    \"\"\"Calculates response rate differences directly from data.\"\"\"\n    prop_1 = data[data['ratio'] == 1]['gave'].mean()\n    prop_2 = data[data['ratio'] == 2]['gave'].mean()\n    prop_3 = data[data['ratio'] == 3]['gave'].mean()\n    return {\n        '1:1 vs. 2:1': prop_2 - prop_1,\n        '2:1 vs. 3:1': prop_3 - prop_2,\n        '1:1 vs. 3:1': prop_3 - prop_1  # Added comparison\n    }\n\ndef regress_and_get_coefficient_differences(data):\n    \"\"\"Regresses 'gave' on ratio dummies and returns coefficient differences.\"\"\"\n    formula = 'gave ~ ratio1 + ratio2 + ratio3'\n    model = sm.ols(formula, data=data).fit()\n    return {\n        '1:1 vs. 2:1': model.params['ratio2'] - model.params['ratio1'],\n        '2:1 vs. 3:1': model.params['ratio3'] - model.params['ratio2'],\n        '1:1 vs. 3:1': model.params['ratio3'] - model.params['ratio1']  # Added comparison\n    }\n\nif __name__ == '__main__':\n    data = pd.read_stata('dataverse_files/AER merged.dta')  # Replace with actual data loading\n    data['ratio1'] = (data['ratio'] == 1).astype(int)\n    data['ratio2'] = (data['ratio'] == 2).astype(int)\n    data['ratio3'] = (data['ratio'] == 3).astype(int)\n\n    direct_diffs = calculate_direct_response_rate_differences(data)\n    regression_diffs = regress_and_get_coefficient_differences(data)\n\n    print(\"Response Rate Differences:\")\n    for key in direct_diffs:\n        print(f\"  {key}: Direct = {direct_diffs[key]:.4f}, Regression = {regression_diffs[key]:.4f}\")\n\n    # Interpretation\n    print(\"\\nInterpretation:\")\n    print(\"--------------------\")\n\n    # Example: Compare the direct and regression differences\n    print(\"Comparison of Direct and Regression Differences:\")\n    for key in direct_diffs:\n        print(f\"  {key}: Difference = {abs(direct_diffs[key] - regression_diffs[key]):.4f}\")\n\n    # Draw conclusions about effectiveness\n    # ... (Your conclusions here based on the compared differences)\n```\n\nThe direct calculations and the differences in regression coefficients yield very similar results. The response rate (proportion donating) increases as the match ratio increases. However, the magnitude of these increases is quite small. The largest difference is between the 1:1 and 3:1 match rates, with a 0.20 percentage point increase in the likelihood of donation. The increase from 1:1 to 2:1 is 0.19 percentage points, and the increase from 2:1 to 3:1 is only 0.01 percentage points.\n\nConclusion on Effectiveness:\n\nWhile there is a trend suggesting that higher match ratios lead to slightly higher donation rates, the small magnitude of these differences, especially the minimal increase from a 2:1 to a 3:1 match, indicates diminishing returns in the effectiveness of increasing the match ratio beyond a certain point. The initial introduction of a match (compared to no match, as seen in the regression intercept) appears to have a more substantial impact than incrementally increasing the generosity of the match.\n\n\n### Size of Charitable Contribution\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n```{python}\nimport pandas as pd\nfrom scipy import stats  \n\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\n# --- Filter data for treatment and control groups ---\ntreatment_amount = data[data['treatment'] == 1]['amount']\ncontrol_amount = data[data['control'] == 1]['amount']\n\n# --- Perform t-test ---\nt_statistic, p_value = stats.ttest_ind(treatment_amount, control_amount)\n\n# --- Print Results ---\nprint(\"T-test Results: Donation Amount by Treatment Status\")\nprint(f\"  T-statistic: {t_statistic:.3f}\")\nprint(f\"  P-value: {p_value:.3f}\")\n\n```\n#### Interpretation:\nThe t-test comparing average donation amounts between the treatment and control groups yielded a t-statistic of 1.861 and a p-value of 0.063. This p-value is slightly above the conventional significance level of 0.05.\n\nTherefore, we do not have strong statistical evidence to conclude that the matching grant offer significantly affected the average amount donated. While the t-statistic suggests a trend towards a higher donation amount in the treatment group, the result is not statistically significant at the 0.05 level.\n\nIn simpler terms, the data doesn't provide conclusive evidence that the matching grant changed how much people gave, on average. It's important to remember that 'not significant' doesn't mean 'no effect'; it simply means we can't rule out that the observed difference occurred by chance.\n\nFurther analysis or a larger sample size might be needed to determine if a real, but subtle, effect exists. This result suggests that the matching grant primarily influenced whether people donated (as shown in previous analyses) rather than dramatically altering the size of individual donations.\n\n_todo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients -- what did we learn? Does the treatment coefficient have a causal interpretation?_ \n#### \n```{python}\nimport pandas as pd\nimport statsmodels.formula.api as sm\n\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\ndonors_data = data[data['gave'] == 1]\n\n# --- Regression on Donors Only ---\nformula_donors = 'amount ~ treatment + ratio2 + ratio3'  # amount as dependent\nmodel_donors = sm.ols(formula_donors, data=donors_data).fit()\n\n# --- Print Results Summary ---\nprint(model_donors.summary())\n\n```\n\n#### Regression Analysis of Donation Amounts (Among Donors)\n\nThis analysis examines the factors influencing the amount donated, focusing only on individuals who made a positive contribution. The OLS regression includes the treatment status (treatment) and the dummy variables for the 2:1 (ratio2) and 3:1 (ratio3) match ratios, with the 1:1 match ratio being the implicit reference category among the match conditions.\n\nThe intercept of 45.5403 suggests that, for those in the control group who donated, the average donation amount was approximately $45.54. This is statistically significant.\n\nThe coefficient for the treatment variable is -0.3974 and is not statistically significant (p = 0.914). This indicates that, among those who donated, there is no statistically significant difference in the average donation amount between individuals who received a matching offer (the treatment group) and those who did not (the control group).\n\nSimilarly, the coefficients for ratio2 (0.1944, p = 0.959) and ratio3 (-3.8911, p = 0.307) are also not statistically significant. This suggests that, among donors, there is no statistically significant difference in the average donation amount between those who received a 2:1 match offer compared to those who received a 1:1 match offer, and no significant difference between those who received a 3:1 match offer compared to those who received a 1:1 match offer.\n\nCausal Interpretation of the Treatment Coefficient:\n\nYes, the coefficient for the treatment variable can be interpreted causally for the subpopulation of individuals who decided to donate. Because the initial assignment to treatment and control groups was random, any systematic differences in the average donation amount among those who donated can likely be attributed to the effect of the treatment.\n\nHowever, it's crucial to remember that this analysis is conditional on someone having already decided to donate. It doesn't tell us about the causal effect of the treatment on the decision to donate (which we analyzed earlier). This coefficient specifically addresses the impact of the matching offer on the amount given, given that a donation occurred. The lack of a significant effect here suggests that the matching offer primarily influenced the extensive margin (whether to donate) rather than the intensive margin (how much to donate), at least at a statistically detectable level within this sample of donors.\n\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\n# --- Filter data for donors only ---\ndonors_data = data[data['gave'] == 1]\n\n# --- Separate data for treatment and control groups among donors ---\ntreatment_donors = donors_data[donors_data['treatment'] == 1]['amount']\ncontrol_donors = donors_data[donors_data['control'] == 1]['amount']\n\n# --- Calculate average donation amounts ---\navg_treatment_donation = treatment_donors.mean()\navg_control_donation = control_donors.mean()\n\n# --- Create the plots ---\nplt.figure(figsize=(12, 6))  # Adjust figure size as needed\n\n# Plot for Treatment Group\nplt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\nplt.hist(treatment_donors, bins=20, alpha=0.7, color='blue')  # Adjust bins as needed\nplt.axvline(avg_treatment_donation, color='red', linestyle='dashed', linewidth=2)  # Add vertical line for average\nplt.title('Donation Amounts - Treatment Group (Donors Only)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.text(avg_treatment_donation + 5, plt.ylim()[1] * 0.9, f\"Avg: {avg_treatment_donation:.2f}\", color='red') #add text\n\n# Plot for Control Group\nplt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\nplt.hist(control_donors, bins=20, alpha=0.7, color='green')  # Adjust bins as needed\nplt.axvline(avg_control_donation, color='red', linestyle='dashed', linewidth=2)  # Add vertical line for average\nplt.title('Donation Amounts - Control Group (Donors Only)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.text(avg_control_donation + 5, plt.ylim()[1] * 0.9, f\"Avg: {avg_control_donation:.2f}\", color='red') #add text\n\nplt.tight_layout()  # Adjust layout to prevent overlap\nplt.show()\n```\n\n## Simulation Experiment\n\nAs a reminder of how the t-statistic \"works,\" in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. \n\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n### Law of Large Numbers\n\n_to do:  Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. This average will likely be \"noisey\" when only averaging a few numbers, but should \"settle down\" and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader._\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Parameters ---\nn_simulations = 10000  # Number of simulated draws\np_control = 0.018     # Probability of donation in the control group\np_treatment = 0.022   # Probability of donation in the treatment group\n\n# --- Simulate Data ---\ncontrol_group = np.random.binomial(1, p_control, n_simulations)  # 1 for donate, 0 for not donate\ntreatment_group = np.random.binomial(1, p_treatment, n_simulations)\n\n# --- Calculate Differences ---\ndifferences = treatment_group - control_group\n\n# --- Calculate Cumulative Average of Differences ---\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n_simulations + 1)\n\n# --- Plotting ---\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg_diff)\nplt.axhline(p_treatment - p_control, color='red', linestyle='--', label='True Treatment Effect')  # True effect line\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Law of Large Numbers Simulation')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# --- Explanation for the reader ---\n```\n\nThis chart demonstrates the Law of Large Numbers in the context of our simulated donation experiment. We simulated 10,000 \"individuals\" in both a control group (donation probability of 0.018) and a treatment group (donation probability of 0.022).  For each simulated individual, we determined whether they donated (1) or not (0) using a random draw from a Bernoulli distribution.\n\nThe blue line in the chart shows the cumulative average difference in donation rates between the treatment and control groups as we include more and more simulated individuals. Initially, with only a few simulations, the average difference is quite variable (\"noisy\") because random fluctuations have a large impact.\n\nHowever, as we simulate more and more individuals, the cumulative average difference converges towards the true treatment effect, which is 0.004 (0.022 - 0.018). This is shown by the red dashed line.\n\nThe Law of Large Numbers tells us that as the sample size increases, the sample average will approach the true population average. In our simulation, the average difference in donation rates eventually settles down to the true difference in donation probabilities between the treatment and control groups. This illustrates that with a sufficiently large sample, we can accurately estimate the true effect of the treatment.\n\n### Central Limit Theorem\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Parameters ---\nn_simulations = 1000  # Number of simulations (repetitions)\nsample_sizes = [50, 200, 500, 1000]  # Sample sizes for the histograms\np_control = 0.018  # Probability of donation in the control group\np_treatment = 0.022  # Probability of donation in the treatment group\n\n# --- Function to simulate and calculate average differences ---\ndef simulate_avg_diff(sample_size, p_control, p_treatment, n_simulations):\n    avg_differences = []\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, sample_size)\n        treatment_sample = np.random.binomial(1, p_treatment, sample_size)\n        avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n        avg_differences.append(avg_diff)\n    return avg_differences\n\n# --- Create the plots ---\nplt.figure(figsize=(12, 8))\nfor i, sample_size in enumerate(sample_sizes):\n    avg_diffs = simulate_avg_diff(sample_size, p_control, p_treatment, n_simulations)\n    plt.subplot(2, 2, i + 1)\n    plt.hist(avg_diffs, bins=20, alpha=0.7, color='purple')  # Adjust bins as needed\n    plt.title(f'Sample Size = {sample_size}')\n    plt.xlabel('Average Difference')\n    plt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n```\n\n\nThis series of four histograms illustrates the Central Limit Theorem. For each histogram, we simulated 1000 experiments. In each experiment, we drew samples from both the control group (donation probability 0.018) and the treatment group (donation probability 0.022) and calculated the difference in the average donation rates between the two groups. We then plotted the distribution of these 1000 average differences.\n\n* **Sample Size = 50:** The first histogram shows the distribution of average differences when we draw samples of size 50 from each group. The distribution is somewhat irregular.\n* **Sample Size = 200:** As we increase the sample size to 200, the distribution becomes smoother and starts to resemble a bell curve.\n* **Sample Size = 500:** With a sample size of 500, the bell curve shape is more pronounced, and the distribution is more concentrated around the true mean difference (0.004).\n* **Sample Size = 1000:** Finally, with a sample size of 1000, the distribution is very close to a normal distribution, tightly centered around the true mean difference.\n\nThe Central Limit Theorem states that the distribution of the sample mean (or average) will approach a normal distribution as the sample size increases, regardless of the shape of the original population distribution (in our case, the Bernoulli distribution).\n\nThese histograms visually demonstrate this theorem. As the sample size grows, the distribution of the average difference becomes more normal, more centered around the true population difference, and less spread out, indicating that our estimates become more precise. This is why statistical tests like the t-test, which rely on the normality assumption, become more reliable with larger sample sizes.\n\n\n\n\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nDean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).\n\nThis project seeks to replicate their results.\n\n### Background\n\nFundraisers often rely on rules of thumb rather than scientific evidence to guide their strategies.  While the economics of charity has been studied from the \"supply\" side (e.g., the impact of tax deductions), there are still \"critical gaps\" in understanding the \"demand\" side (i.e., how and why donors choose to give).    \n\nOne common fundraising tool is the matching grant, where a donor pledges to match the contributions of others.  Fundraisers often believe that higher matching ratios (e.g., 2:1 or 3:1) are more effective in attracting donations, but there is limited research to support this. \n\n### Purpose of the Study (Research Question)\n\nKarlan and List (2007) aimed to address this gap by examining the impact of matching grants on charitable giving through a large-scale natural field experiment.  Specifically, the study investigates whether and to what extent \"price\" (in the form of matching grant ratios) matters in charitable fundraising.    \n\n### Background of the charity\n\nThe experiment was conducted in collaboration with a liberal non-profit organization in the United States that focuses on social and policy issues related to civil liberties.  The organization is a 501(c)3 charity, meaning donations are tax-deductible.  The organization regularly solicits donations from its prior donors through direct mail campaigns.    \n\nThe organization's membership has the following characteristics:\n\n1. Approximately 70% male\n2. Approximately 60% over 65 years old\n3. Approximately 80% with a college education\n4. Political leaning: 85% voted for Gore in the 2000 presidential election \n\n### Experimental Design\n\nThe researchers conducted a natural field experiment using a direct mail solicitation to over 50,000 past donors of the organization.  Individuals were randomly assigned to either a treatment group that received a matching grant offer or a control group that did not.  The treatment group was further randomized to receive different matching grant ratios. \n\n### What Was the Treatment Received by the Groups\n\nAll participants received a four-page fundraising letter.    \n\nControl Group: Received a standard letter with no mention of a matching grant.    \nTreatment Groups: Received a letter that included an additional paragraph announcing the matching grant offer.  The matching grant had different price ratios:\n1:1 match: For every dollar donated, the organization receives two dollars.    \n2:1 match: For every dollar donated, the organization receives three dollars.    \n3:1 match: For every dollar donated, the organization receives four dollars.\n\n\n![](additionalParagraph.png)\n\n\n## Data\n\n### Description\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n| Variable             | Description                                                         |\n|----------------------|---------------------------------------------------------------------|\n| `treatment`          | Treatment                                                           |\n| `control`            | Control                                                             |\n| `ratio`              | Match ratio                                                         |\n| `ratio2`             | 2:1 match ratio                                                     |\n| `ratio3`             | 3:1 match ratio                                                     |\n| `size`               | Match threshold                                                     |\n| `size25`             | \\$25,000 match threshold                                            |\n| `size50`             | \\$50,000 match threshold                                            |\n| `size100`            | \\$100,000 match threshold                                           |\n| `sizeno`             | Unstated match threshold                                            |\n| `ask`                | Suggested donation amount                                           |\n| `askd1`              | Suggested donation was highest previous contribution                |\n| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |\n| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |\n| `ask1`               | Highest previous contribution (for suggestion)                      |\n| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |\n| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |\n| `amount`             | Dollars given                                                       |\n| `gave`               | Gave anything                                                       |\n| `amountchange`       | Change in amount given                                              |\n| `hpa`                | Highest previous contribution                                       |\n| `ltmedmra`           | Small prior donor: last gift was less than median \\$35              |\n| `freq`               | Number of prior donations                                           |\n| `years`              | Number of years since initial donation                              |\n| `year5`              | At least 5 years since initial donation                             |\n| `mrm2`               | Number of months since last donation                                |\n| `dormant`            | Already donated in 2005                                             |\n| `female`             | Female                                                              |\n| `couple`             | Couple                                                              |\n| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |\n| `nonlit`             | Nonlitigation                                                       |\n| `cases`              | Court cases from state in 2004-5 in which organization was involved |\n| `statecnt`           | Percent of sample from state                                        |\n| `stateresponse`      | Proportion of sample from the state who gave                        |\n| `stateresponset`     | Proportion of treated sample from the state who gave                |\n| `stateresponsec`     | Proportion of control sample from the state who gave                |\n| `stateresponsetminc` | stateresponset - stateresponsec                                     |\n| `perbush`            | State vote share for Bush                                           |\n| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |\n| `red0`               | Red state                                                           |\n| `blue0`              | Blue state                                                          |\n| `redcty`             | Red county                                                          |\n| `bluecty`            | Blue county                                                         |\n| `pwhite`             | Proportion white within zip code                                    |\n| `pblack`             | Proportion black within zip code                                    |\n| `page18_39`          | Proportion age 18-39 within zip code                                |\n| `ave_hh_sz`          | Average household size within zip code                              |\n| `median_hhincome`    | Median household income within zip code                             |\n| `powner`             | Proportion house owner within zip code                              |\n| `psch_atlstba`       | Proportion who finished college within zip code                     |\n| `pop_propurban`      | Proportion of population urban within zip code                      |\n\n::::\n\n```{python}\nimport pandas as pd\n\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\n#print(data.head())\n\nprint(data.info())\n\nprint(data.describe())\n```\n\n\n### Balance Tests: Checking the success of randomization\n\nIn any experiment that uses random assignment, it's crucial to verify that the randomization process was successful in creating comparable groups. This ensures that any differences I observe in donation behavior are likely due to the matching grant offers and not simply due to pre-existing differences between the groups. To do this, I conducted balance tests on several pre-treatment variables.\n\n#### Methodology\n\nTo assess the balance between the treatment and control groups, I performed two types of statistical tests for each pre-treatment variable:\n\n- T-tests: I used independent samples t-tests to compare the means of each variable between the treatment and control groups. The null hypothesis for each t-test was that there is no difference in the average value of the variable between the two groups.\n\n- Linear Regressions: I also ran a series of simple linear regressions. In each regression, the pre-treatment variable was the dependent variable, and the treatment assignment (coded as 0 for control and 1 for treatment) was the independent variable. This allowed me to estimate the average difference in the pre-treatment variable between the treatment and control groups.\n\nIt's important to note that, as the professor emphasized, the t-test and the regression approach are two ways of examining the same thing and should give me very similar results.\n\n#### Results\n\nI conducted these balance tests on the following pre-treatment variables:\n\n1. Months since last donation\n2. Number of prior donations\n3. Female (an indicator variable for gender)\n4. Couple (an indicator variable for whether the individual is part of a couple)\n\nThe code for this, as well as the results from the balance tests are as follows: \n\n```{python code-fold=\"true\"}\nimport pandas as pd\nfrom scipy import stats\nimport statsmodels.formula.api as sm\nimport numpy as np\n\n# Load your data\ndata = pd.read_stata('dataverse_files/AER merged.dta')  # Or pd.read_csv(), etc.\n\n#print(data[['treatment', 'control']].head(2))\ndef test_balance(var_value, var_name):\n\n    print('\\n')\n    variable_to_test = var_value\n\n    # T-test\n    control_group = data[data['control'] == 1][variable_to_test]\n    treatment_group = data[data['treatment'] == 1][variable_to_test]\n    #print(treatment_group.head(2))\n    #print(control_group.head(2))\n\n    # Calculate means\n    mean_control = np.mean(control_group)\n    mean_treatment = np.mean(treatment_group)\n\n    # Calculate variances\n    variance_control = np.var(control_group, ddof=1)  # ddof=1 for sample variance\n    variance_treatment = np.var(treatment_group, ddof=1)\n\n    # Calculate sample sizes\n    n_control = len(control_group)\n    n_treatment = len(treatment_group)\n\n    # Calculate pooled variance\n    pooled_variance = ((n_treatment - 1) * variance_treatment + (n_control - 1) * variance_control) / (n_treatment + n_control - 2)\n\n    # Calculate pooled standard error\n    pooled_standard_error = np.sqrt(pooled_variance * (1 / n_treatment + 1 / n_control))\n\n    # Calculate t-statistic\n    t_statistic_manual = (mean_treatment - mean_control) / pooled_standard_error\n    print(f\"t-statistic for {var_name}: t = {t_statistic_manual:.3f}\")\n\n    df = n_treatment + n_control - 2\n\n    p_value_manual_two_tailed = stats.t.sf(abs(t_statistic_manual), df) * 2\n\n    print(f\"Two-Tailed p-value: {p_value_manual_two_tailed:.3f}\")\n\n\n\n    # Linear Regression\n    formula = f'{variable_to_test} ~ treatment'\n    model = sm.ols(formula, data=data).fit()\n    p_value_reg = model.pvalues['treatment']\n    print(f\"Regression for {var_name}: p = {p_value_reg:.3f}\")\n\n    # Interpretation\n    if p_value_manual_two_tailed < 0.05:\n        print(f\"For {var_name}, there is a statistically significant difference between treatment and control groups.\")\n    else:\n        print(f\"For {var_name}, there is no statistically significant difference between treatment and control groups.\")\n\n\ntest_balance('mrm2', 'Months since last donation')\ntest_balance('freq', 'Number of Prior Donations')\n#test_balance('perbush', 'State vote share for Bush')\n#test_balance('cases', 'Court cases from 2004-05 in which organization was involved')\ntest_balance('female', 'Female')\ntest_balance('couple', 'Couple')\n# Repeat for other variables\n```\n\n#### Overall Assessment of Randomization\n\nBased on these balance tests, I conclude that the randomization process in the Karlan and List (2007) experiment appears to have been largely successful. For all four pre-treatment variables I examined, I did not find statistically significant differences between the treatment and control groups at the 0.05 significance level.\n\nWhile some minor differences are expected due to chance, especially in large samples, the general lack of statistical significance suggests that the treatment and control groups are indeed comparable. This strengthens the internal validity of the study and supports the authors' ability to attribute any observed differences in donation behavior to the matching grant offers.\n\n#### Importance of Balance Tests\n\nBalance tests are a critical component of any experimental study, and Karlan and List (2007) included them for good reason. These tests serve several important purposes:\n\n- Establishing Internal Validity: They provide evidence that the treatment, rather than pre-existing differences, is the likely cause of any observed effects.\n- Supporting Causal Inference: They justify making causal claims about the impact of the treatment.\n- Ensuring Rigor: They demonstrate the rigor of the experimental design and increase confidence in the study's findings.\n- Promoting Transparency: They allow other researchers to assess the quality of the experiment and attempt to replicate the results.\n\n## Experimental Results\n\n### Charitable Contribution Made\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. \n\n#### Barplot for proportion of people who donated\n```{python code-fold=\"true\"}\nimport pandas as pd\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\nimport plotly.graph_objects as go\n\ndef plot_donation_by_group(data):\n    # 2. Calculate donation proportions\n    proportion_gave_treatment = data[data['treatment'] == 1]['gave'].mean()\n    proportion_gave_control = data[data['control'] == 1]['gave'].mean()\n\n    # 3. Prepare data for plotting\n    plot_data = pd.DataFrame({\n        'group': ['Treatment', 'Control'],\n        'proportion_gave': [proportion_gave_treatment, proportion_gave_control]\n    })\n\n    # 4. Create the bar plot\n    fig = go.Figure(data=[\n        go.Bar(x=plot_data['group'], y=plot_data['proportion_gave'])\n    ])\n\n    fig.add_trace(go.Scatter(\n        x=plot_data['group'],\n        y=plot_data['proportion_gave'],\n        text=[f\"{100 * p:.2f}%\" for p in plot_data['proportion_gave']],  # Format proportions\n        mode='text',\n        textposition='top center'\n    ))\n    # 5. Customize the plot\n    fig.update_layout(\n        title='Proportion of Donors by Group',\n        xaxis_title='Group',\n        yaxis_title='Proportion Donated',\n        yaxis_range=[0, max(plot_data['proportion_gave']) * 1.1]  # Extend y-axis slightly\n    )\n\n    # 6. Display the plot\n    fig.show()\n\n\nplot_donation_by_group(data)\n```\n\nThe results of my analysis show that the treatment group, which received the matching grant offer, had a higher donation rate (2.2%) compared to the control group (1.79%). This suggests that the matching grant may have had a positive impact on the likelihood of individuals making a donation. However, it's important to keep in mind that this is just a preliminary observation, and further statistical testing is needed to determine if this difference is statistically significant.\n\n#### Binary donation outcome: t-test and it's interpretation:\n```{python}\nimport pandas as pd\nfrom scipy import stats  # For t-test\n\ndef perform_t_test_on_donation(data):\n    \"\"\"\n    Performs an independent samples t-test on the binary donation outcome.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'treatment', 'control', and 'gave' columns.\n\n    Returns:\n        tuple: (t-statistic, p-value)\n    \"\"\"\n    # --- T-test ---\n    treatment_gave = data[data['treatment'] == 1]['gave']\n    control_gave = data[data['control'] == 1]['gave']\n    t_statistic, p_value_ttest = stats.ttest_ind(treatment_gave, control_gave)\n\n    return t_statistic, p_value_ttest\n\n\nif __name__ == '__main__':\n    # --- Load Your Actual Data Here ---\n    data = pd.read_stata('dataverse_files/AER merged.dta')  # Replace with your data loading\n\n    t_statistic, p_value = perform_t_test_on_donation(data)\n\n    print(\"--- T-test Results ---\")\n    print(f\"T-statistic: {t_statistic:.3f}\")\n    print(f\"P-value: {p_value:.3f}\")\n\n    # You would then proceed to interpret these results\n```\n\n#### Interpretation of the t-test:\n\nThe t-test results indicate a statistically significant difference in donation behavior between the group that received the matching grant offer (the treatment group) and the group that did not (the control group). The p-value of 0.002 is substantially lower than the typical significance level of 0.05, meaning that the observed difference is unlikely to have occurred by chance alone. In other words, there's strong evidence to suggest that the matching grant offer had a real effect on whether people decided to donate.\n\nThe positive t-statistic (3.101) suggests that the treatment group had a higher donation rate. This finding aligns with the idea that individuals respond to price-like incentives, even in charitable giving. The matching grant effectively reduces the 'cost' of giving; for every dollar a person donates, the charity receives more. This increased 'bang for your buck' appears to motivate people to be more generous.\n\nFrom a behavioral perspective, this result highlights that while altruism and a desire to support a cause are important motivators for charitable giving, financial incentives can also play a significant role. People seem to weigh the perceived value of their contribution, and matching grants can significantly shift that calculation. This underscores the potential effectiveness of matching grant campaigns as a fundraising strategy.\n\n#### Probit regression: Results and interpretation\n\n```{python}\nimport pandas as pd\nimport statsmodels.api as sm  # For Probit regression\n\ndef perform_probit_regression(data):\n    \"\"\"\n    Performs a probit regression to model the probability of donation.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'treatment', 'control', and 'gave' columns.\n\n    Returns:\n        sm.ProbitResults: The results of the probit regression.\n    \"\"\"\n\n    # --- Verify that 'treatment' and 'control' are mutually exclusive ---\n    # --- Create treatment dummy variable ---\n    data['treatment_dummy'] = data['treatment']  # Assuming 'treatment' is 1/0\n\n    # --- Probit Regression ---\n    formula = 'gave ~ treatment_dummy'\n    probit_model = sm.Probit(data['gave'], sm.add_constant(data['treatment_dummy']))\n    probit_results = probit_model.fit()\n\n    return probit_results\n\n\nif __name__ == '__main__':\n    data = pd.read_stata('dataverse_files/AER merged.dta')\n    probit_results = perform_probit_regression(data)\n    \n    # --- Print Results Summary ---\n    print(probit_results.summary())\n\n    # --- Access specific results ---\n    print(\"\\nProbit Regression Results:\")\n    print(f\"Coefficient on treatment: {probit_results.params['treatment_dummy']:.4f}\")\n    print(f\"P-value on treatment: {probit_results.pvalues['treatment_dummy']:.4f}\")\n```\n\n#### Interpretation of the results:\n\nThe probit regression results indicate a statistically significant positive effect of the matching grant offer on the probability of making a charitable donation. The coefficient on the treatment_dummy variable is 0.0868, and it has a p-value of 0.002, which is well below the conventional significance level of 0.05. This suggests that being offered a matching grant significantly increased the likelihood that an individual would donate.\n\nTo understand the magnitude of this effect, we need to consider that the coefficients in a probit regression represent the change in the z-score of the standard normal distribution associated with a one-unit change in the predictor variable (in this case, going from the control group to the treatment group). While the coefficient itself isn't a direct measure of the change in probability, its positive and statistically significant value clearly demonstrates that the matching offer had a positive impact on the propensity to give.\n\nIn behavioral terms, this finding reinforces the idea that the 'price' mechanism introduced by the matching grant influences donation decisions. The offer of having their donation matched appears to have provided an additional incentive, nudging individuals who received the treatment letter towards donating at a higher rate than those in the control group who did not receive such an offer. This aligns with economic theories suggesting that individuals respond to incentives, even in the realm of altruistic behavior.\n\n### Differences between Match Rates\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n_todo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the \"figures suggest\" comment the authors make on page 8?_\n\n#### Pairwise t-tests: Results\n```{python}\nimport pandas as pd\nfrom scipy import stats  # For t-test\n\ndef compare_donation_rates_by_ratio(data):\n    \"\"\"\n    Compares donation rates across different match ratios using t-tests.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'ratio' and 'gave' columns.\n\n    Returns:\n        dict: A dictionary containing the t-test results for each comparison.\n    \"\"\"\n\n    results = {}\n\n    # 1:1 vs. 2:1\n    gave_1 = data[data['ratio'] == 1]['gave']\n    gave_2 = data[data['ratio'] == 2]['gave']\n    t_stat, p_value = stats.ttest_ind(gave_1, gave_2)\n    results['1:1 vs. 2:1'] = {'t_statistic': t_stat, 'p_value': p_value}\n\n    # 1:1 vs. 3:1\n    gave_1 = data[data['ratio'] == 1]['gave']\n    gave_3 = data[data['ratio'] == 3]['gave']\n    t_stat, p_value = stats.ttest_ind(gave_1, gave_3)\n    results['1:1 vs. 3:1'] = {'t_statistic': t_stat, 'p_value': p_value}\n\n    # 2:1 vs. 3:1\n    gave_2 = data[data['ratio'] == 2]['gave']\n    gave_3 = data[data['ratio'] == 3]['gave']\n    t_stat, p_value = stats.ttest_ind(gave_2, gave_3)\n    results['2:1 vs. 3:1'] = {'t_statistic': t_stat, 'p_value': p_value}\n\n    return results\n\n\nif __name__ == '__main__':\n    # --- Load Your Actual Data Here ---\n    data = pd.read_stata('dataverse_files/AER merged.dta')  # Replace with your data loading\n\n    results = compare_donation_rates_by_ratio(data)\n\n    # --- Print Results ---\n    print(\"T-test Results: Donation Rates by Match Ratio\")\n    for comparison, values in results.items():\n        print(f\"\\nComparison: {comparison}\")\n        print(f\"  T-statistic: {values['t_statistic']:.3f}\")\n        print(f\"  P-value: {values['p_value']:.3f}\")\n```\n\n#### Pairwise t-tests: Interpretation:\n**Lack of Statistical Significance:**\n\n- For all three comparisons, the p-values are considerably greater than the common significance level of 0.05.\n- This means that we fail to reject the null hypothesis in each case. The null hypothesis states that there is no difference in the average donation rates between the two groups being compared.\n- In simpler terms, the data does not provide enough evidence to conclude that changing the match ratio (from 1:1 to 2:1, from 1:1 to 3:1, or from 2:1 to 3:1) has a statistically significant impact on the proportion of people who donate.\n- These results match the conclusions made in the original research publication.\n\n**Behavioral Implications:**\n\n- These findings might suggest that, within the range of match ratios tested, the specific generosity of the match offer doesn't strongly influence the likelihood of donating.\n- It's possible that the presence of any match is more important than the size of the match. People might be motivated by the simple fact that their donation will be multiplied, without being very sensitive to the exact multiplication factor.\n- Another possibility is that the differences in match ratios tested were not large enough to elicit a statistically detectable change in behavior.\n\n\n\n```{python}\nimport statsmodels.formula.api as sm  # For OLS regression\n\nimport pandas as pd\ndata = pd.read_stata('dataverse_files/AER merged.dta')\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\n\n# --- Regression ---\nformula = 'gave ~ ratio1 + ratio2 + ratio3'\nmodel = sm.ols(formula, data=data).fit()\n\n# --- Print Results Summary ---\nprint(model.summary())\n\n# --- Interpretation ---\nprint(\"\\nRegression Results Interpretation:\")\nprint(\"----------------------------\")\n\nprint(f\"Coefficient for ratio1: {model.params['ratio1']:.4f}\")\nprint(f\"P-value for ratio1: {model.pvalues['ratio1']:.4f}\")\n\nprint(f\"Coefficient for ratio2: {model.params['ratio2']:.4f}\")\nprint(f\"P-value for ratio2: {model.pvalues['ratio2']:.4f}\")\n\nprint(f\"Coefficient for ratio3: {model.params['ratio3']:.4f}\")\nprint(f\"P-value for ratio3: {model.pvalues['ratio3']:.4f}\")\n\n\n```\n\nInterpretation of the OLS Regression Results\n\nIn this OLS regression, we modeled the binary outcome of whether a donation was made (gave) as a function of dummy variables representing the different match ratios (ratio1 for 1:1, ratio2 for 2:1, and ratio3 for 3:1). The \"Control\" group (no match) serves as the implicit reference category.\n\nHere's a breakdown of the key findings:\n\nIntercept (Coefficient: 0.0179, P-value: 0.000): The intercept represents the predicted probability of donation for the reference group, which is the \"Control\" group (where ratio1, ratio2, and ratio3 are all 0). A coefficient of 0.0179 suggests that in the absence of any matching offer, the predicted donation rate is approximately 1.79%. This is statistically significant (p < 0.001).\n\nCoefficient for ratio1 (0.0029, P-value: 0.097): This coefficient estimates the additional change in the probability of donation when the match ratio is 1:1, compared to the Control group. The positive coefficient (0.0029) suggests that the 1:1 match is associated with a 0.29 percentage point increase in the predicted donation rate (1.79% + 0.29% = 2.08%). However, the p-value of 0.097 is greater than the conventional significance level of 0.05. Therefore, we do not have statistically significant evidence that the 1:1 matching offer significantly increased donations compared to the control group in this OLS model.\n\nCoefficient for ratio2 (0.0048, P-value: 0.006): This coefficient estimates the additional change in the probability of donation when the match ratio is 2:1, compared to the Control group. The positive and statistically significant coefficient (0.0048, p = 0.006) indicates that the 2:1 matching offer led to a significant increase of 0.48 percentage points in the predicted donation rate compared to the Control group (1.79% + 0.48% = 2.27%).\n\nCoefficient for ratio3 (0.0049, P-value: 0.005): This coefficient estimates the additional change in the probability of donation when the match ratio is 3:1, compared to the Control group. The positive and statistically significant coefficient (0.0049, p = 0.005) shows that the 3:1 matching offer resulted in a significant increase of 0.49 percentage points in the predicted donation rate compared to the Control group (1.79% + 0.49% = 2.28%).\n\nOverall Interpretation:\n\nThe OLS regression results suggest that while a 1:1 matching offer did not significantly increase donation rates compared to no offer, both the 2:1 and 3:1 matching offers had a statistically significant positive impact on the likelihood of donation. The magnitudes of the effects for the 2:1 and 3:1 matches are quite similar, suggesting that increasing the match ratio beyond 2:1 might not lead to a substantial further increase in the proportion of donors.\n\nThese findings are broadly consistent with the idea that matching grants incentivize charitable giving. The stronger effects observed for the higher match ratios (2:1 and 3:1) provide some evidence that the \"price\" of giving matters, with a more generous match leading to a greater increase in donation propensity compared to no match. However, the lack of a significant effect for the 1:1 match hints at a possible threshold effect or a weaker influence at that particular ratio in this model.\n\n\n```{python}\nimport pandas as pd\nimport statsmodels.formula.api as sm\n\ndef calculate_direct_response_rate_differences(data):\n    \"\"\"Calculates response rate differences directly from data.\"\"\"\n    prop_1 = data[data['ratio'] == 1]['gave'].mean()\n    prop_2 = data[data['ratio'] == 2]['gave'].mean()\n    prop_3 = data[data['ratio'] == 3]['gave'].mean()\n    return {\n        '1:1 vs. 2:1': prop_2 - prop_1,\n        '2:1 vs. 3:1': prop_3 - prop_2,\n        '1:1 vs. 3:1': prop_3 - prop_1  # Added comparison\n    }\n\ndef regress_and_get_coefficient_differences(data):\n    \"\"\"Regresses 'gave' on ratio dummies and returns coefficient differences.\"\"\"\n    formula = 'gave ~ ratio1 + ratio2 + ratio3'\n    model = sm.ols(formula, data=data).fit()\n    return {\n        '1:1 vs. 2:1': model.params['ratio2'] - model.params['ratio1'],\n        '2:1 vs. 3:1': model.params['ratio3'] - model.params['ratio2'],\n        '1:1 vs. 3:1': model.params['ratio3'] - model.params['ratio1']  # Added comparison\n    }\n\nif __name__ == '__main__':\n    data = pd.read_stata('dataverse_files/AER merged.dta')  # Replace with actual data loading\n    data['ratio1'] = (data['ratio'] == 1).astype(int)\n    data['ratio2'] = (data['ratio'] == 2).astype(int)\n    data['ratio3'] = (data['ratio'] == 3).astype(int)\n\n    direct_diffs = calculate_direct_response_rate_differences(data)\n    regression_diffs = regress_and_get_coefficient_differences(data)\n\n    print(\"Response Rate Differences:\")\n    for key in direct_diffs:\n        print(f\"  {key}: Direct = {direct_diffs[key]:.4f}, Regression = {regression_diffs[key]:.4f}\")\n\n    # Interpretation\n    print(\"\\nInterpretation:\")\n    print(\"--------------------\")\n\n    # Example: Compare the direct and regression differences\n    print(\"Comparison of Direct and Regression Differences:\")\n    for key in direct_diffs:\n        print(f\"  {key}: Difference = {abs(direct_diffs[key] - regression_diffs[key]):.4f}\")\n\n    # Draw conclusions about effectiveness\n    # ... (Your conclusions here based on the compared differences)\n```\n\nThe direct calculations and the differences in regression coefficients yield very similar results. The response rate (proportion donating) increases as the match ratio increases. However, the magnitude of these increases is quite small. The largest difference is between the 1:1 and 3:1 match rates, with a 0.20 percentage point increase in the likelihood of donation. The increase from 1:1 to 2:1 is 0.19 percentage points, and the increase from 2:1 to 3:1 is only 0.01 percentage points.\n\nConclusion on Effectiveness:\n\nWhile there is a trend suggesting that higher match ratios lead to slightly higher donation rates, the small magnitude of these differences, especially the minimal increase from a 2:1 to a 3:1 match, indicates diminishing returns in the effectiveness of increasing the match ratio beyond a certain point. The initial introduction of a match (compared to no match, as seen in the regression intercept) appears to have a more substantial impact than incrementally increasing the generosity of the match.\n\n\n### Size of Charitable Contribution\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n```{python}\nimport pandas as pd\nfrom scipy import stats  \n\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\n# --- Filter data for treatment and control groups ---\ntreatment_amount = data[data['treatment'] == 1]['amount']\ncontrol_amount = data[data['control'] == 1]['amount']\n\n# --- Perform t-test ---\nt_statistic, p_value = stats.ttest_ind(treatment_amount, control_amount)\n\n# --- Print Results ---\nprint(\"T-test Results: Donation Amount by Treatment Status\")\nprint(f\"  T-statistic: {t_statistic:.3f}\")\nprint(f\"  P-value: {p_value:.3f}\")\n\n```\n#### Interpretation:\nThe t-test comparing average donation amounts between the treatment and control groups yielded a t-statistic of 1.861 and a p-value of 0.063. This p-value is slightly above the conventional significance level of 0.05.\n\nTherefore, we do not have strong statistical evidence to conclude that the matching grant offer significantly affected the average amount donated. While the t-statistic suggests a trend towards a higher donation amount in the treatment group, the result is not statistically significant at the 0.05 level.\n\nIn simpler terms, the data doesn't provide conclusive evidence that the matching grant changed how much people gave, on average. It's important to remember that 'not significant' doesn't mean 'no effect'; it simply means we can't rule out that the observed difference occurred by chance.\n\nFurther analysis or a larger sample size might be needed to determine if a real, but subtle, effect exists. This result suggests that the matching grant primarily influenced whether people donated (as shown in previous analyses) rather than dramatically altering the size of individual donations.\n\n_todo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients -- what did we learn? Does the treatment coefficient have a causal interpretation?_ \n#### \n```{python}\nimport pandas as pd\nimport statsmodels.formula.api as sm\n\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\ndonors_data = data[data['gave'] == 1]\n\n# --- Regression on Donors Only ---\nformula_donors = 'amount ~ treatment + ratio2 + ratio3'  # amount as dependent\nmodel_donors = sm.ols(formula_donors, data=donors_data).fit()\n\n# --- Print Results Summary ---\nprint(model_donors.summary())\n\n```\n\n#### Regression Analysis of Donation Amounts (Among Donors)\n\nThis analysis examines the factors influencing the amount donated, focusing only on individuals who made a positive contribution. The OLS regression includes the treatment status (treatment) and the dummy variables for the 2:1 (ratio2) and 3:1 (ratio3) match ratios, with the 1:1 match ratio being the implicit reference category among the match conditions.\n\nThe intercept of 45.5403 suggests that, for those in the control group who donated, the average donation amount was approximately $45.54. This is statistically significant.\n\nThe coefficient for the treatment variable is -0.3974 and is not statistically significant (p = 0.914). This indicates that, among those who donated, there is no statistically significant difference in the average donation amount between individuals who received a matching offer (the treatment group) and those who did not (the control group).\n\nSimilarly, the coefficients for ratio2 (0.1944, p = 0.959) and ratio3 (-3.8911, p = 0.307) are also not statistically significant. This suggests that, among donors, there is no statistically significant difference in the average donation amount between those who received a 2:1 match offer compared to those who received a 1:1 match offer, and no significant difference between those who received a 3:1 match offer compared to those who received a 1:1 match offer.\n\nCausal Interpretation of the Treatment Coefficient:\n\nYes, the coefficient for the treatment variable can be interpreted causally for the subpopulation of individuals who decided to donate. Because the initial assignment to treatment and control groups was random, any systematic differences in the average donation amount among those who donated can likely be attributed to the effect of the treatment.\n\nHowever, it's crucial to remember that this analysis is conditional on someone having already decided to donate. It doesn't tell us about the causal effect of the treatment on the decision to donate (which we analyzed earlier). This coefficient specifically addresses the impact of the matching offer on the amount given, given that a donation occurred. The lack of a significant effect here suggests that the matching offer primarily influenced the extensive margin (whether to donate) rather than the intensive margin (how much to donate), at least at a statistically detectable level within this sample of donors.\n\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_stata('dataverse_files/AER merged.dta')\n\n# --- Filter data for donors only ---\ndonors_data = data[data['gave'] == 1]\n\n# --- Separate data for treatment and control groups among donors ---\ntreatment_donors = donors_data[donors_data['treatment'] == 1]['amount']\ncontrol_donors = donors_data[donors_data['control'] == 1]['amount']\n\n# --- Calculate average donation amounts ---\navg_treatment_donation = treatment_donors.mean()\navg_control_donation = control_donors.mean()\n\n# --- Create the plots ---\nplt.figure(figsize=(12, 6))  # Adjust figure size as needed\n\n# Plot for Treatment Group\nplt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\nplt.hist(treatment_donors, bins=20, alpha=0.7, color='blue')  # Adjust bins as needed\nplt.axvline(avg_treatment_donation, color='red', linestyle='dashed', linewidth=2)  # Add vertical line for average\nplt.title('Donation Amounts - Treatment Group (Donors Only)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.text(avg_treatment_donation + 5, plt.ylim()[1] * 0.9, f\"Avg: {avg_treatment_donation:.2f}\", color='red') #add text\n\n# Plot for Control Group\nplt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\nplt.hist(control_donors, bins=20, alpha=0.7, color='green')  # Adjust bins as needed\nplt.axvline(avg_control_donation, color='red', linestyle='dashed', linewidth=2)  # Add vertical line for average\nplt.title('Donation Amounts - Control Group (Donors Only)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.text(avg_control_donation + 5, plt.ylim()[1] * 0.9, f\"Avg: {avg_control_donation:.2f}\", color='red') #add text\n\nplt.tight_layout()  # Adjust layout to prevent overlap\nplt.show()\n```\n\n## Simulation Experiment\n\nAs a reminder of how the t-statistic \"works,\" in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. \n\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n### Law of Large Numbers\n\n_to do:  Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. This average will likely be \"noisey\" when only averaging a few numbers, but should \"settle down\" and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader._\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Parameters ---\nn_simulations = 10000  # Number of simulated draws\np_control = 0.018     # Probability of donation in the control group\np_treatment = 0.022   # Probability of donation in the treatment group\n\n# --- Simulate Data ---\ncontrol_group = np.random.binomial(1, p_control, n_simulations)  # 1 for donate, 0 for not donate\ntreatment_group = np.random.binomial(1, p_treatment, n_simulations)\n\n# --- Calculate Differences ---\ndifferences = treatment_group - control_group\n\n# --- Calculate Cumulative Average of Differences ---\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n_simulations + 1)\n\n# --- Plotting ---\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg_diff)\nplt.axhline(p_treatment - p_control, color='red', linestyle='--', label='True Treatment Effect')  # True effect line\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Law of Large Numbers Simulation')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# --- Explanation for the reader ---\n```\n\nThis chart demonstrates the Law of Large Numbers in the context of our simulated donation experiment. We simulated 10,000 \"individuals\" in both a control group (donation probability of 0.018) and a treatment group (donation probability of 0.022).  For each simulated individual, we determined whether they donated (1) or not (0) using a random draw from a Bernoulli distribution.\n\nThe blue line in the chart shows the cumulative average difference in donation rates between the treatment and control groups as we include more and more simulated individuals. Initially, with only a few simulations, the average difference is quite variable (\"noisy\") because random fluctuations have a large impact.\n\nHowever, as we simulate more and more individuals, the cumulative average difference converges towards the true treatment effect, which is 0.004 (0.022 - 0.018). This is shown by the red dashed line.\n\nThe Law of Large Numbers tells us that as the sample size increases, the sample average will approach the true population average. In our simulation, the average difference in donation rates eventually settles down to the true difference in donation probabilities between the treatment and control groups. This illustrates that with a sufficiently large sample, we can accurately estimate the true effect of the treatment.\n\n### Central Limit Theorem\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Parameters ---\nn_simulations = 1000  # Number of simulations (repetitions)\nsample_sizes = [50, 200, 500, 1000]  # Sample sizes for the histograms\np_control = 0.018  # Probability of donation in the control group\np_treatment = 0.022  # Probability of donation in the treatment group\n\n# --- Function to simulate and calculate average differences ---\ndef simulate_avg_diff(sample_size, p_control, p_treatment, n_simulations):\n    avg_differences = []\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, sample_size)\n        treatment_sample = np.random.binomial(1, p_treatment, sample_size)\n        avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n        avg_differences.append(avg_diff)\n    return avg_differences\n\n# --- Create the plots ---\nplt.figure(figsize=(12, 8))\nfor i, sample_size in enumerate(sample_sizes):\n    avg_diffs = simulate_avg_diff(sample_size, p_control, p_treatment, n_simulations)\n    plt.subplot(2, 2, i + 1)\n    plt.hist(avg_diffs, bins=20, alpha=0.7, color='purple')  # Adjust bins as needed\n    plt.title(f'Sample Size = {sample_size}')\n    plt.xlabel('Average Difference')\n    plt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n```\n\n\nThis series of four histograms illustrates the Central Limit Theorem. For each histogram, we simulated 1000 experiments. In each experiment, we drew samples from both the control group (donation probability 0.018) and the treatment group (donation probability 0.022) and calculated the difference in the average donation rates between the two groups. We then plotted the distribution of these 1000 average differences.\n\n* **Sample Size = 50:** The first histogram shows the distribution of average differences when we draw samples of size 50 from each group. The distribution is somewhat irregular.\n* **Sample Size = 200:** As we increase the sample size to 200, the distribution becomes smoother and starts to resemble a bell curve.\n* **Sample Size = 500:** With a sample size of 500, the bell curve shape is more pronounced, and the distribution is more concentrated around the true mean difference (0.004).\n* **Sample Size = 1000:** Finally, with a sample size of 1000, the distribution is very close to a normal distribution, tightly centered around the true mean difference.\n\nThe Central Limit Theorem states that the distribution of the sample mean (or average) will approach a normal distribution as the sample size increases, regardless of the shape of the original population distribution (in our case, the Bernoulli distribution).\n\nThese histograms visually demonstrate this theorem. As the sample size grows, the distribution of the average difference becomes more normal, more centered around the true population difference, and less spread out, indicating that our estimates become more precise. This is why statistical tests like the t-test, which rely on the normality assumption, become more reliable with larger sample sizes.\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.5","theme":["cosmo","brand"],"title":"A Replication of Karlan and List (2007)","author":"Sumedh Hambarde","date":"today","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}