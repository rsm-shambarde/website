{"title":"Poisson Regression Examples","markdown":{"yaml":{"title":"Poisson Regression Examples","author":"Sumedh Hambarde","date":"today","callout-appearance":"minimal"},"headingText":"Blueprinty Case Study","containsRefs":false,"markdown":"\n\n\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty hus collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Replace 'your_data.csv' with the actual path to your data file\ndata = pd.read_csv('blueprinty.csv')\n\n# It's always a good idea to get a quick look at the data\nprint(data.head())\nprint(data.info())\nprint(data.describe())\n# The column indicating Blueprinty usage is 'iscustomer' (0 for no, 1 for yes)\ncustomer_users = data[data['iscustomer'] == 1]\nnon_customers = data[data['iscustomer'] == 0]\n\n# The column with the number of patents is 'patents'\npatents_column = 'patents'\n\n# Create histograms\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.histplot(customer_users[patents_column], kde=True)\nplt.title('Number of Patents (Blueprinty Customers)')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nsns.histplot(non_customers[patents_column], kde=True)\nplt.title('Number of Patents (Non-Customers)')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate means\nmean_patents_customer = customer_users[patents_column].mean()\nmean_patents_non_customer = non_customers[patents_column].mean()\n\nprint(f\"\\nMean number of patents (Blueprinty customers): {mean_patents_customer:.2f}\")\nprint(f\"Mean number of patents (Non-customers): {mean_patents_non_customer:.2f}\")\n```\n\n\nThe histograms reveal that the distribution of patents awarded is right-skewed for both Blueprinty customers and non-customers, but the distribution for customers appears shifted towards a higher number of patents, with a peak around 4-5 compared to 2-3 for non-customers; this visual difference is supported by the mean number of patents, which is 4.13 for customers and 3.47 for non-customers, suggesting that, on average, Blueprinty customers have a higher number of patents, although further statistical modeling is needed to confirm the significance and account for other factors.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport io\ndata = pd.read_csv('blueprinty.csv')\n\n# Assuming you have already read your data into a pandas DataFrame named 'data'\n# If not, uncomment and replace 'your_data.csv' with the actual path\n# data = pd.read_csv('your_data.csv')\n\n# --- Comparing Regions by Customer Status ---\nregion_customer_table = pd.crosstab(data['region'], data['iscustomer'])\nprint(\"Region vs. Customer Status Contingency Table:\\n\", region_customer_table.to_string())\n\n# Create a stacked bar chart (this will show a plot)\nregion_customer_table.plot(kind='bar', stacked=True, figsize=(8, 6))\nplt.title('Regional Distribution by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Number of Firms')\nplt.xticks(rotation=0)\nplt.legend(title='Is Customer', labels=['No', 'Yes'])\nplt.show()\nplt.close() # Close the plot to avoid it showing up again later\n\n# --- Comparing Ages by Customer Status ---\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.histplot(data[data['iscustomer'] == 1]['age'], kde=True)\nplt.title('Age of Firms (Blueprinty Customers)')\nplt.xlabel('Age (Years)')\nplt.ylabel('Frequency')\nplt.show()\nplt.close()\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 2)\nsns.histplot(data[data['iscustomer'] == 0]['age'], kde=True)\nplt.title('Age of Firms (Non-Customers)')\nplt.xlabel('Age (Years)')\nplt.ylabel('Frequency')\nplt.show()\nplt.close()\n\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='iscustomer', y='age', data=data)\nplt.title('Age of Firms by Customer Status')\nplt.xlabel('Is Customer')\nplt.ylabel('Age (Years)')\nplt.xticks([0, 1], ['No', 'Yes'])\nplt.show()\nplt.close()\n\nage_summary = data.groupby('iscustomer')['age'].describe()\nprint(\"\\nAge Summary Statistics:\\n\", age_summary.to_string())\n\n# --- Comparing Patents (as done previously) ---\ncustomer_users = data[data['iscustomer'] == 1]\nnon_customers = data[data['iscustomer'] == 0]\npatents_column = 'patents'\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.histplot(customer_users[patents_column], kde=True)\nplt.title('Number of Patents (Blueprinty Customers)')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.show()\nplt.close()\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 2)\nsns.histplot(non_customers[patents_column], kde=True)\nplt.title('Number of Patents (Non-Customers)')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.show()\nplt.close()\n\nmean_patents_customer = customer_users[patents_column].mean()\nmean_patents_non_customer = non_customers[patents_column].mean()\n\nprint(f\"\\nMean number of patents (Blueprinty customers): {mean_patents_customer:.2f}\")\nprint(f\"Mean number of patents (Non-customers): {mean_patents_non_customer:.2f}\")\n```\n\n#### Observations from the Region vs. Customer Status Contingency Table:\n\nTotal Firms per Region:\n\nMidwest: 187 (non-customer) + 37 (customer) = 224\nNortheast: 273 (non-customer) + 328 (customer) = 601\nNorthwest: 158 (non-customer) + 29 (customer) = 187\nSouth: 156 (non-customer) + 35 (customer) = 191\nSouthwest: 245 (non-customer) + 52 (customer) = 297\nDistribution of Customers:\n\nThe Northeast region has the highest number of customers (328), significantly more than any other region.\nThe Southwest region has the second-highest number of customers (52).\nThe Midwest and South regions have a similar, lower number of customers (37 and 35, respectively).\nThe Northwest has the fewest customers (29).\nDistribution of Non-Customers:\n\nThe Northeast also has the highest number of non-customers (273).\nThe Southwest has the second-highest number of non-customers (245).\nThe Midwest has a substantial number of non-customers (187).\nThe South and Northwest have a lower number of non-customers (156 and 158, respectively).\nProportion of Customers within each Region: To get a better sense of whether customers are disproportionately located in certain regions, we can calculate the percentage of customers in each region:\n\nMidwest: (37 / 224) * 100% ≈ 16.5%\nNortheast: (328 / 601) * 100% ≈ 54.6%\nNorthwest: (29 / 187) * 100% ≈ 15.5%\nSouth: (35 / 191) * 100% ≈ 18.3%\nSouthwest: (52 / 297) * 100% ≈ 17.5%\nObservations on Proportions:\n\nThe proportion of Blueprinty customers is strikingly higher in the Northeast region (around 54.6%) compared to all other regions, where the proportion ranges from approximately 15.5% to 18.3%.\nPotential Implications for the Marketing Claim:\n\nThe strong concentration of Blueprinty customers in the Northeast region raises a crucial point. If the Northeast region is inherently more innovative or has a higher propensity for patent applications due to other factors (e.g., industry concentration, research institutions), then the higher number of patents among Blueprinty customers might be partly or even largely attributable to their regional location rather than solely due to the software.\nTo properly assess the effect of Blueprinty, it will be important to control for the \"region\" variable in any statistical models you build.\n\n#### Observations on Firm Age by Customer Status:\n\nMean Age: The mean age of Blueprinty customers (26.90 years) is slightly higher than the mean age of non-customers (26.10 years). The difference is about 0.8 years. While this difference exists, it doesn't appear to be dramatically large.\nMedian Age (50%): The median age of customers (26.5 years) is also slightly higher than that of non-customers (25.5 years), with a difference of 1 year. This suggests that the \"typical\" customer firm might be marginally older.\nAge Range (Min and Max): The minimum and maximum ages are quite similar for both groups, ranging from around 9-10 years to 47.5-49 years. This indicates that both younger and older firms can be found in both customer and non-customer categories.\nInterquartile Range (25% and 75%):\nFor non-customers, the interquartile range (IQR) is 31.25 - 21.0 = 10.25 years.\nFor customers, the IQR is 32.50 - 20.5 = 12.0 years. The slightly larger IQR for customers suggests a bit more variability in the middle 50% of their age distribution.\nStandard Deviation: The standard deviation of age is also slightly higher for customers (7.81 years) compared to non-customers (6.95 years), indicating a slightly wider spread in the ages of customer firms overall.\nPotential Implications for the Marketing Claim:\n\nThe fact that Blueprinty customers are, on average, slightly older might be a factor to consider. Older firms might have more established patenting processes or be in industries with a higher propensity to patent. However, the difference in average age is less than a year, so it's unlikely to be the sole driver of any observed differences in patent numbers.\nThe distributions of age for both groups seem relatively similar, with a central tendency in the mid-twenties.\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nThe likelihood function for a single observation $Y$ from a Poisson distribution with parameter $\\lambda$ is:\n\n$$L(\\lambda | Y) = f(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}$$\n\nThe log-likelihood function is:\n\n$$\\ell(\\lambda | Y) = \\ln(L(\\lambda | Y)) = -\\lambda + Y \\ln(\\lambda) - \\ln(Y!)$$\n\n\n\n```{python}\nimport numpy as np\nfrom math import factorial\nimport pandas as pd\ndata = pd.read_csv('blueprinty.csv')\n\n\ndef poisson_loglikelihood(lambd, Y):\n  if lambd <= 0 or not Y == int(Y) or Y < 0:\n    return -np.inf\n  return -lambd + Y * np.log(lambd) - np.log(factorial(Y))\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom math import factorial\n\n# Use the mean number of patents from the entire dataset as Y\nmean_patents_overall = data['patents'].mean()\nY_plot = int(round(mean_patents_overall))\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 2 * mean_patents_overall, 200)\n\n# Calculate log-likelihood values\nloglikelihoods = [poisson_loglikelihood(lambd, Y_plot) for lambd in lambda_values]\n\n# Plot lambda vs. log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, loglikelihoods)\nplt.title(f'Log-Likelihood vs. Lambda (Y = {Y_plot})')\nplt.xlabel('Lambda (λ)')\nplt.ylabel('Log-Likelihood ℓ(λ|Y)')\nplt.grid(True)\nplt.show()\n```\n\n\n\n#### Interpretation of the Log-Likelihood Plot:\n\nThe plot shows the log-likelihood of observing Y=4 (the rounded mean number of patents in our dataset) for different values of the Poisson parameter λ. The curve exhibits a clear peak. This peak represents the value of λ that maximizes the log-likelihood (and thus the likelihood) of observing our data. In this case, the log-likelihood appears to be maximized when λ is approximately 4. This visual confirms that the Maximum Likelihood Estimate (MLE) for λ, given a single observation equal to the sample mean, is the sample mean itself.\n\n\nIf we take the log-likelihood function for a sample of $n$ i.i.d. Poisson observations $y_1, y_2, ..., y_n$:\n\n$$\\ell(\\lambda | y_1, ..., y_n) = -n\\lambda + \\left(\\sum_{i=1}^{n} y_i\\right) \\ln(\\lambda) - \\sum_{i=1}^{n} \\ln(y_i!)$$\n\nTo find the MLE for $\\lambda$, we take the first derivative of the log-likelihood with respect to $\\lambda$ and set it to zero:\n\n$$\\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = -n + \\frac{\\sum_{i=1}^{n} y_i}{\\lambda} = 0$$\n\nSolving for $\\lambda$:\n\n$$\\frac{\\sum_{i=1}^{n} y_i}{\\lambda} = n$$\n\n$$\\lambda = \\frac{\\sum_{i=1}^{n} y_i}{n} = \\bar{Y}$$\n\nThus, the Maximum Likelihood Estimate ($\\lambda_{MLE}$) for the Poisson parameter $\\lambda$ is indeed the sample mean ($\\bar{Y}$). This result aligns with the property that the mean of a Poisson distribution is equal to its parameter $\\lambda$.\n\n\nTo find the Maximum Likelihood Estimate (MLE) for the Poisson parameter λ using numerical optimization, we can use the minimize function from the scipy.optimize library in Python. We define the negative of the log-likelihood function (since minimize performs minimization) for our observed number of patents. By providing an initial guess for λ (such as the sample mean) and running the optimization, we can find the value of λ that minimizes the negative log-likelihood, which is equivalent to maximizing the log-likelihood.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom math import factorial\nfrom scipy.optimize import minimize\n\ndata = pd.read_csv('blueprinty.csv')\nY_observed = data['patents'].values  # Get all observed number of patents\n\ndef negative_poisson_loglikelihood(lambd, Y):\n  \"\"\"Calculates the negative log-likelihood for the entire sample.\"\"\"\n  if lambd <= 0:\n    return np.inf\n  log_likelihood = np.sum(-lambd + Y * np.log(lambd) - np.log(np.array([factorial(y) for y in Y])))\n  return -log_likelihood\n\n# Initial guess for lambda (e.g., the sample mean)\ninitial_lambda = np.mean(Y_observed)\n\n# Perform the optimization\nresult = minimize(negative_poisson_loglikelihood, initial_lambda, args=(Y_observed,), method='L-BFGS-B', bounds=[(0.001, None)])\n\n# Extract the MLE estimate for lambda\nmle_lambda = result.x[0]\n\nprint(f\"Maximum Likelihood Estimate (MLE) for lambda: {mle_lambda:.4f}\")\nprint(f\"Sample Mean of Patents: {initial_lambda:.4f}\")\n```\n\nUsing numerical optimization with the minimize function from scipy.optimize in Python, we found the Maximum Likelihood Estimate (MLE) for the Poisson parameter λ to be approximately 3.6847. This value is remarkably close to the sample mean of the number of patents in our dataset, which is also 3.6847. This empirical result reinforces the theoretical finding that the MLE of λ for a Poisson distribution is the sample mean.\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom math import factorial\nfrom scipy.optimize import minimize\nimport statsmodels.api as sm\n\ndata = pd.read_csv('blueprinty.csv')\nY_observed = data['patents'].values\n\n# Prepare the covariate matrix X, ensuring numeric types\ndata['age_squared'] = pd.to_numeric(data['age'], errors='coerce')**2\nX = pd.get_dummies(data, columns=['region'], drop_first=True)\nX['age'] = pd.to_numeric(X['age'], errors='coerce')\nX['age_squared'] = pd.to_numeric(X['age_squared'], errors='coerce')\nX['iscustomer'] = pd.to_numeric(X['iscustomer'], errors='coerce')\nX = X[['age', 'age_squared', 'iscustomer', 'region_Northeast', 'region_Northwest', 'region_South', 'region_Southwest']].values\n# Add a constant term (intercept) to the matrix X\nX = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1).astype(float)\n\ndef negative_poisson_regression_loglikelihood(beta, Y, X):\n  \"\"\"Calculates the negative log-likelihood for Poisson regression.\"\"\"\n  if np.any(np.isnan(beta)):\n    return np.inf\n  log_lambda = np.dot(X, beta)\n  lambda_i = np.exp(log_lambda)\n  log_likelihood = np.sum(-lambda_i + Y * log_lambda - np.log(np.array([factorial(y) for y in Y])))\n  return -log_likelihood\n\n# Initial guess for beta (zeros), ensuring float type\ninitial_beta = np.zeros(X.shape[1], dtype=float)\n\n# Perform the optimization to find MLE\nresult = minimize(negative_poisson_regression_loglikelihood, initial_beta, args=(Y_observed, X), method='BFGS', jac=None, hess=None)\n\n# Extract the MLE estimates for beta\nmle_beta = result.x\n\n# --- Using statsmodels for standard errors ---\npoisson_model = sm.GLM(Y_observed, X, family=sm.families.Poisson()).fit()\nstandard_errors = poisson_model.bse\ncoefficients = poisson_model.params\n\n# Create a table of coefficients and standard errors\nresults_table = pd.DataFrame({\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors\n})\n\nprint(\"\\nTable of Coefficients and Standard Errors (using statsmodels):\\n\", results_table)\n```\n#### Interpretation of Poisson Regression Coefficients:\n\nWe estimated a Poisson regression model to understand the relationship between the number of patents awarded and firm characteristics, including age, age squared, region, and whether the firm is a Blueprinty customer (iscustomer). \n\n#### Key Observations:\n\nIntercept: The intercept is estimated at -0.509.\nAge: The positive coefficient for age (0.149) suggests that, holding other factors constant, older firms tend to have a higher expected number of patents.\nAge Squared: The negative coefficient for age squared (-0.003) indicates a potential non-linear relationship with age, suggesting that the positive effect of age on patents might diminish at higher ages.\nBlueprinty Customer (iscustomer): The positive and statistically significant coefficient for iscustomer (0.208) is of primary interest. To interpret this, we exponentiate the coefficient: exp(0.208)≈1.23. This suggests that, holding other factors constant, Blueprinty customers have an estimated expected number of patents that is about 23% higher than non-customers.\nRegion: The coefficients for the region dummy variables (Northeast, Northwest, South, Southwest) are relative to the baseline region (Midwest). For example, firms in the Northeast have an estimated expected number of patents that is exp(0.029)≈1.03 times that of firms in the Midwest, after controlling for other variables. The significance of these regional effects would need to be assessed based on their standard errors.\nThese results provide evidence that, even after controlling for age and region, Blueprinty customers tend to have a higher expected number of patents compared to non-customers in this dataset.\n\n\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.genmod import families\n\ndata = pd.read_csv('blueprinty.csv')\n\n# Prepare the covariate matrix X\ndata['age_squared'] = data['age']**2\nX = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n# Select the columns for our model\nX_cols = ['age', 'age_squared', 'iscustomer', 'region_Northeast', 'region_Northwest', 'region_South', 'region_Southwest']\nX = X[X_cols].copy() # Use .copy() to avoid SettingWithCopyWarning\n\n# Explicitly convert X columns to float64\nfor col in X.columns:\n    X[col] = pd.to_numeric(X[col], errors='coerce')\n\n# Add a constant term (intercept) to the matrix X\nX = sm.add_constant(X, prepend=True)\n\n# Ensure the constant is also float\nX['const'] = pd.to_numeric(X['const'], errors='coerce')\n\nY_observed = data['patents']\n\n# Try converting X to numpy array with float dtype\nX_array = np.asarray(X, dtype=float)\n\n# Fit the Poisson GLM\ntry:\n    poisson_glm_model = sm.GLM(Y_observed, X_array, family=families.Poisson()).fit()\n    # Print the summary of the model\n    print(poisson_glm_model.summary())\nexcept ValueError as e:\n    print(f\"ValueError during model fitting: {e}\")\n```\n\n#### Interpretation of the results:\n\nThe results of the Poisson regression model indicate that firm age, age squared, and being a Blueprinty customer are statistically significant predictors of the number of patents awarded. The positive coefficient for iscustomer suggests that, even after controlling for age and regional location, Blueprinty customers have a significantly higher expected number of patents (approximately 23% higher) compared to non-customers. The effects of the different regions compared to the Midwest baseline were not found to be statistically significant in this model.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.genmod import families\n\ndata = pd.read_csv('blueprinty.csv')\n\n# Prepare the covariate matrix X\ndata['age_squared'] = data['age']**2\nX = pd.get_dummies(data, columns=['region'], drop_first=True)\nX = X[['age', 'age_squared', 'iscustomer', 'region_Northeast', 'region_Northwest', 'region_South', 'region_Southwest']].copy()\n\n# Explicitly convert X columns to float64\nfor col in X.columns:\n    X[col] = pd.to_numeric(X[col], errors='coerce').astype(float)\n\n# Add a constant term (intercept) to the matrix X\nX = sm.add_constant(X, prepend=True)\nX['const'] = pd.to_numeric(X['const'], errors='coerce').astype(float)\n\nY_observed = data['patents']\n\n# Fit the Poisson GLM\npoisson_glm_model = sm.GLM(Y_observed, X, family=families.Poisson()).fit()\n\n# Print model summary (for inspection)\nprint(\"\\nPoisson GLM Model Summary before predict:\\n\", poisson_glm_model.summary())\n\n# Print columns of X\nprint(\"\\nColumns of X before creating X_0 and X_1:\\n\", X.columns)\n\n# Create X_0: X data with iscustomer = 0 for all observations\nX_0 = X.copy()\nX_0['iscustomer'] = float(0)\nprint(\"\\nX_0 data types before predict:\\n\", X_0.dtypes)\nprint(\"\\nX_0 head before predict:\\n\", X_0.head())\nprint(\"\\nNumPy array dtype of X_0:\\n\", np.asarray(X_0).dtype)\n\n# Create X_1: X data with iscustomer = 1 for all observations\nX_1 = X.copy()\nX_1['iscustomer'] = float(1)\nprint(\"\\nX_1 data types before predict:\\n\", X_1.dtypes)\nprint(\"\\nX_1 head before predict:\\n\", X_1.head())\nprint(\"\\nNumPy array dtype of X_1:\\n\", np.asarray(X_1).dtype)\n\n# Try predicting on the original X\ntry:\n    log_y_pred_original = poisson_glm_model.predict(X)\n    print(\"\\nPrediction on original X successful.\")\nexcept ValueError as e:\n    print(f\"\\nValueError during prediction on original X: {e}\")\n\n# Predict the log of the expected number of patents for X_0 and X_1\ntry:\n    log_y_pred_0 = poisson_glm_model.predict(X_0)\n    log_y_pred_1 = poisson_glm_model.predict(X_1)\n\n    # The predicted number of patents is exp(log_y_pred)\n    y_pred_0 = np.exp(log_y_pred_0)\n    y_pred_1 = np.exp(log_y_pred_1)\n\n    # Calculate the difference in predicted number of patents\n    difference = y_pred_1 - y_pred_0\n\n    # Calculate the average difference\n    average_difference = np.mean(difference)\n\n    print(f\"\\nAverage predicted number of patents (if all were customers): {np.mean(y_pred_1):.2f}\")\n    print(f\"Average predicted number of patents (if all were non-customers): {np.mean(y_pred_0):.2f}\")\n    print(f\"Average difference in predicted number of patents due to Blueprinty: {average_difference:.2f}\")\n\nexcept ValueError as e:\n    print(f\"ValueError during prediction: {e}\")\n```\n#### Interpretation:\n\nBased on our analysis, the average predicted number of patents over five years for firms in our dataset, if they were all Blueprinty customers, is approximately 83.10. In contrast, the average predicted number of patents if none of them were customers is about 35.49. The average difference between these two scenarios is 47.61 patents. This suggests a substantial positive effect of using Blueprinty's software on patent success, as our model predicts that, on average, firms using the software are expected to be awarded approximately 47.61 more patents over five years compared to non-users, while holding age and regional location constant at their observed levels.\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n####  EDA\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the Airbnb dataset\nairbnb_data = pd.read_csv('airbnb.csv') \n\n# Basic information about the dataset\nprint(\"Airbnb Data Info:\")\nairbnb_data.info()\n\n# Summary statistics for numerical variables\nprint(\"\\nAirbnb Data Summary Statistics:\")\nprint(airbnb_data.describe())\n\n# Look at the distribution of the proxy for bookings (number_of_reviews)\nplt.figure(figsize=(10, 6))\nsns.histplot(airbnb_data['number_of_reviews'], bins=50, kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.show()\n\n# Explore the relationship between room_type and number_of_reviews\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='room_type', y='number_of_reviews', data=airbnb_data)\nplt.title('Number of Reviews by Room Type')\nplt.xlabel('Room Type')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n# Explore the relationship between price and number_of_reviews (might be non-linear)\nplt.figure(figsize=(10, 6))\nplt.scatter(airbnb_data['price'], airbnb_data['number_of_reviews'], alpha=0.3)\nplt.title('Number of Reviews vs. Price')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.xlim(0, 1000) # Limit price for better visualization\nplt.ylim(0, 200)  # Limit reviews for better visualization\nplt.show()\n\n# Explore the distribution of review scores\nreview_scores_cols = ['review_scores_cleanliness', 'review_scores_location', 'review_scores_value']\nairbnb_data[review_scores_cols].hist(figsize=(12, 4), bins=10)\nplt.suptitle('Distribution of Review Scores', y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Explore the 'instant_bookable' variable\nprint(\"\\nValue counts for instant_bookable:\")\nprint(airbnb_data['instant_bookable'].value_counts())\n```\n#### Handling Missing Values\n```{python}\nimport pandas as pd\nimport numpy as np\n\n# Load the Airbnb dataset\nairbnb_data = pd.read_csv('airbnb.csv')\n\n# Convert 'last_scraped' and 'host_since' to datetime objects\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Calculate 'days' the unit has been listed\nairbnb_data['days'] = (airbnb_data['last_scraped'] - airbnb_data['host_since']).dt.days\n\n# Identify relevant columns for modeling\nrelevant_cols = ['number_of_reviews', 'room_type', 'bathrooms', 'bedrooms', 'price',\n                 'review_scores_cleanliness', 'review_scores_location',\n                 'review_scores_value', 'instant_bookable', 'days']\n\n# Check for missing values in the relevant columns\nmissing_values = airbnb_data[relevant_cols].isnull().sum()\nprint(\"Missing values in relevant columns:\\n\", missing_values)\n\n# Handle missing values\n# Option 1: Drop rows with any missing values in the relevant columns\nairbnb_data_cleaned = airbnb_data[relevant_cols].dropna()\nprint(\"\\nShape of data before dropping missing values:\", airbnb_data[relevant_cols].shape)\nprint(\"Shape of data after dropping missing values:\", airbnb_data_cleaned.shape)\n\n# Option 2: Impute missing review scores (e.g., with the mean or median)\n# You might choose this if you want to retain more data, but for simplicity,\n# we'll proceed with dropping rows for now.\n\n# Verify that there are no missing values in the cleaned data\nprint(\"\\nMissing values in cleaned data:\\n\", airbnb_data_cleaned.isnull().sum())\n```\n\n\n#### Poisson Regression Model\n```{python}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.genmod import families\n\n# Load the Airbnb dataset\nairbnb_data = pd.read_csv('airbnb.csv')\n\n# Convert 'last_scraped' and 'host_since' to datetime objects\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Calculate 'days' the unit has been listed\nairbnb_data['days'] = (airbnb_data['last_scraped'] - airbnb_data['host_since']).dt.days\n\n# Define relevant columns and drop rows with missing values\nrelevant_cols = ['number_of_reviews', 'room_type', 'bathrooms', 'bedrooms', 'price',\n                 'review_scores_cleanliness', 'review_scores_location',\n                 'review_scores_value', 'instant_bookable', 'days']\nairbnb_data_cleaned = airbnb_data[relevant_cols].dropna().copy()\n\n# Convert categorical variables to dummy variables\nairbnb_data_cleaned = pd.get_dummies(airbnb_data_cleaned, columns=['room_type', 'instant_bookable'], drop_first=True)\n\n# Define the dependent variable (number_of_reviews) and independent variables\ny = airbnb_data_cleaned['number_of_reviews']\nX = airbnb_data_cleaned.drop('number_of_reviews', axis=1).copy() # Use .copy()\n\n# Explicitly convert all columns in X to float64\nfor col in X.columns:\n    X[col] = pd.to_numeric(X[col], errors='coerce').astype(float)\n\n# Add a constant to the independent variables for the intercept\nX = sm.add_constant(X, prepend=True)\nX['const'] = pd.to_numeric(X['const'], errors='coerce').astype(float)\n\n# Check the data types of X before fitting\nprint(\"Data types of X before fitting GLM:\\n\", X.dtypes)\nprint(\"\\nHead of X before fitting GLM:\\n\", X.head())\nprint(\"\\nData type of y before fitting GLM:\\n\", y.dtype)\nprint(\"\\nHead of y before fitting GLM:\\n\", y.head())\n\n# Fit the Poisson regression model\ntry:\n    poisson_model = sm.GLM(y, X, family=families.Poisson()).fit()\n    # Print the model summary\n    print(poisson_model.summary())\nexcept ValueError as e:\n    print(f\"ValueError during model fitting: {e}\")\n```\n\n#### Interpretation of the Poisson Regression Coefficients\n\nThe coefficients in a Poisson regression model (with a log link function) represent the change in the log of the expected outcome (number of reviews) for a one-unit increase in the predictor variable, holding other predictors constant. To interpret the effect on the expected number of reviews itself, we need to exponentiate the coefficients. exp(β) gives the multiplicative factor by which the expected number of reviews changes.\n\nHere's a breakdown of the coefficients:\n\nconst (-0.6881): This is the intercept. When all other predictors are zero, the log of the expected number of reviews is -0.6881. exp(−0.6881)≈0.50. This baseline might not be practically meaningful as some predictors (like review scores) cannot be zero.\n\nbathrooms (0.0817): For each additional bathroom, the log of the expected number of reviews increases by 0.0817. exp(0.0817)≈1.085. So, holding other factors constant, each additional bathroom is associated with an approximately 8.5% increase in the expected number of reviews. This effect is statistically significant (p < 0.001).\n\nbedrooms (0.0484): For each additional bedroom, the log of the expected number of reviews increases by 0.0484. exp(0.0484)≈1.050. Each additional bedroom is associated with an approximately 5.0% increase in the expected number of reviews (significant, p < 0.001).\n\nprice (-0.0007): For each one-dollar increase in price, the log of the expected number of reviews decreases by 0.0007. exp(−0.0007)≈0.9993. A one-dollar increase in price is associated with a very small (0.07%) decrease in the expected number of reviews (significant, p < 0.001).\n\nreview_scores_cleanliness (0.3284): For each one-point increase in the cleanliness score (on a 1-10 scale), the log of the expected number of reviews increases by 0.3284. exp(0.3284)≈1.389. A one-point increase in cleanliness score is associated with an approximately 38.9% increase in the expected number of reviews (significant, p < 0.001).\n\nreview_scores_location (0.1528): For each one-point increase in the location score, the log of the expected number of reviews increases by 0.1528. exp(0.1528)≈1.165. A one-point increase in location score is associated with an approximately 16.5% increase in the expected number of reviews (significant, p < 0.001).\n\nreview_scores_value (0.2192): For each one-point increase in the value score, the log of the expected number of reviews increases by 0.2192. exp(0.2192)≈1.245. A one-point increase in value score is associated with an approximately 24.5% increase in the expected number of reviews (significant, p < 0.001).\n\ndays (0.0004): For each additional day the unit has been listed, the log of the expected number of reviews increases by 0.0004. exp(0.0004)≈1.0004. Each additional day listed is associated with a very small (0.04%) increase in the expected number of reviews (significant, p < 0.001).\n\nroom_type_Private room (-0.2640): Compared to an entire home/apartment (the baseline), private rooms have a lower expected number of reviews. exp(−0.2640)≈0.768. Private rooms are associated with approximately 23.2% fewer expected reviews (significant, p < 0.001).\n\nroom_type_Shared room (-1.1152): Compared to an entire home/apartment, shared rooms have a much lower expected number of reviews. exp(−1.1152)≈0.328. Shared rooms are associated with approximately 67.2% fewer expected reviews (significant, p < 0.001).\n\ninstant_bookable_t (0.4591): Listings that are instantly bookable have a higher expected number of reviews. exp(0.4591)≈1.582. Instantly bookable listings are associated with approximately 58.2% more expected reviews (significant, p < 0.001).\n\n#### Summary:\n\nThe Poisson regression model reveals several factors associated with the number of reviews (our proxy for bookings) of Airbnb listings in New York City. Listings with more bathrooms and bedrooms tend to have more reviews. Higher prices are associated with a very slight decrease in reviews. Review scores, particularly for cleanliness, location, and value, show a strong positive association with the number of reviews. The longer a listing has been active (more days), the slightly more reviews it tends to have. Compared to entire homes/apartments, private and especially shared rooms tend to have significantly fewer reviews. Finally, listings that are instantly bookable have a substantially higher number of reviews. All these effects are statistically significant at the p < 0.001 level.\n\nThis analysis provides insights into what factors might drive the popularity (as measured by the number of reviews) of Airbnb listings in NYC.\n\n\n\n","srcMarkdownNoYaml":"\n\n\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty hus collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Replace 'your_data.csv' with the actual path to your data file\ndata = pd.read_csv('blueprinty.csv')\n\n# It's always a good idea to get a quick look at the data\nprint(data.head())\nprint(data.info())\nprint(data.describe())\n# The column indicating Blueprinty usage is 'iscustomer' (0 for no, 1 for yes)\ncustomer_users = data[data['iscustomer'] == 1]\nnon_customers = data[data['iscustomer'] == 0]\n\n# The column with the number of patents is 'patents'\npatents_column = 'patents'\n\n# Create histograms\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.histplot(customer_users[patents_column], kde=True)\nplt.title('Number of Patents (Blueprinty Customers)')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nsns.histplot(non_customers[patents_column], kde=True)\nplt.title('Number of Patents (Non-Customers)')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate means\nmean_patents_customer = customer_users[patents_column].mean()\nmean_patents_non_customer = non_customers[patents_column].mean()\n\nprint(f\"\\nMean number of patents (Blueprinty customers): {mean_patents_customer:.2f}\")\nprint(f\"Mean number of patents (Non-customers): {mean_patents_non_customer:.2f}\")\n```\n\n\nThe histograms reveal that the distribution of patents awarded is right-skewed for both Blueprinty customers and non-customers, but the distribution for customers appears shifted towards a higher number of patents, with a peak around 4-5 compared to 2-3 for non-customers; this visual difference is supported by the mean number of patents, which is 4.13 for customers and 3.47 for non-customers, suggesting that, on average, Blueprinty customers have a higher number of patents, although further statistical modeling is needed to confirm the significance and account for other factors.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport io\ndata = pd.read_csv('blueprinty.csv')\n\n# Assuming you have already read your data into a pandas DataFrame named 'data'\n# If not, uncomment and replace 'your_data.csv' with the actual path\n# data = pd.read_csv('your_data.csv')\n\n# --- Comparing Regions by Customer Status ---\nregion_customer_table = pd.crosstab(data['region'], data['iscustomer'])\nprint(\"Region vs. Customer Status Contingency Table:\\n\", region_customer_table.to_string())\n\n# Create a stacked bar chart (this will show a plot)\nregion_customer_table.plot(kind='bar', stacked=True, figsize=(8, 6))\nplt.title('Regional Distribution by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Number of Firms')\nplt.xticks(rotation=0)\nplt.legend(title='Is Customer', labels=['No', 'Yes'])\nplt.show()\nplt.close() # Close the plot to avoid it showing up again later\n\n# --- Comparing Ages by Customer Status ---\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.histplot(data[data['iscustomer'] == 1]['age'], kde=True)\nplt.title('Age of Firms (Blueprinty Customers)')\nplt.xlabel('Age (Years)')\nplt.ylabel('Frequency')\nplt.show()\nplt.close()\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 2)\nsns.histplot(data[data['iscustomer'] == 0]['age'], kde=True)\nplt.title('Age of Firms (Non-Customers)')\nplt.xlabel('Age (Years)')\nplt.ylabel('Frequency')\nplt.show()\nplt.close()\n\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='iscustomer', y='age', data=data)\nplt.title('Age of Firms by Customer Status')\nplt.xlabel('Is Customer')\nplt.ylabel('Age (Years)')\nplt.xticks([0, 1], ['No', 'Yes'])\nplt.show()\nplt.close()\n\nage_summary = data.groupby('iscustomer')['age'].describe()\nprint(\"\\nAge Summary Statistics:\\n\", age_summary.to_string())\n\n# --- Comparing Patents (as done previously) ---\ncustomer_users = data[data['iscustomer'] == 1]\nnon_customers = data[data['iscustomer'] == 0]\npatents_column = 'patents'\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.histplot(customer_users[patents_column], kde=True)\nplt.title('Number of Patents (Blueprinty Customers)')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.show()\nplt.close()\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 2)\nsns.histplot(non_customers[patents_column], kde=True)\nplt.title('Number of Patents (Non-Customers)')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.show()\nplt.close()\n\nmean_patents_customer = customer_users[patents_column].mean()\nmean_patents_non_customer = non_customers[patents_column].mean()\n\nprint(f\"\\nMean number of patents (Blueprinty customers): {mean_patents_customer:.2f}\")\nprint(f\"Mean number of patents (Non-customers): {mean_patents_non_customer:.2f}\")\n```\n\n#### Observations from the Region vs. Customer Status Contingency Table:\n\nTotal Firms per Region:\n\nMidwest: 187 (non-customer) + 37 (customer) = 224\nNortheast: 273 (non-customer) + 328 (customer) = 601\nNorthwest: 158 (non-customer) + 29 (customer) = 187\nSouth: 156 (non-customer) + 35 (customer) = 191\nSouthwest: 245 (non-customer) + 52 (customer) = 297\nDistribution of Customers:\n\nThe Northeast region has the highest number of customers (328), significantly more than any other region.\nThe Southwest region has the second-highest number of customers (52).\nThe Midwest and South regions have a similar, lower number of customers (37 and 35, respectively).\nThe Northwest has the fewest customers (29).\nDistribution of Non-Customers:\n\nThe Northeast also has the highest number of non-customers (273).\nThe Southwest has the second-highest number of non-customers (245).\nThe Midwest has a substantial number of non-customers (187).\nThe South and Northwest have a lower number of non-customers (156 and 158, respectively).\nProportion of Customers within each Region: To get a better sense of whether customers are disproportionately located in certain regions, we can calculate the percentage of customers in each region:\n\nMidwest: (37 / 224) * 100% ≈ 16.5%\nNortheast: (328 / 601) * 100% ≈ 54.6%\nNorthwest: (29 / 187) * 100% ≈ 15.5%\nSouth: (35 / 191) * 100% ≈ 18.3%\nSouthwest: (52 / 297) * 100% ≈ 17.5%\nObservations on Proportions:\n\nThe proportion of Blueprinty customers is strikingly higher in the Northeast region (around 54.6%) compared to all other regions, where the proportion ranges from approximately 15.5% to 18.3%.\nPotential Implications for the Marketing Claim:\n\nThe strong concentration of Blueprinty customers in the Northeast region raises a crucial point. If the Northeast region is inherently more innovative or has a higher propensity for patent applications due to other factors (e.g., industry concentration, research institutions), then the higher number of patents among Blueprinty customers might be partly or even largely attributable to their regional location rather than solely due to the software.\nTo properly assess the effect of Blueprinty, it will be important to control for the \"region\" variable in any statistical models you build.\n\n#### Observations on Firm Age by Customer Status:\n\nMean Age: The mean age of Blueprinty customers (26.90 years) is slightly higher than the mean age of non-customers (26.10 years). The difference is about 0.8 years. While this difference exists, it doesn't appear to be dramatically large.\nMedian Age (50%): The median age of customers (26.5 years) is also slightly higher than that of non-customers (25.5 years), with a difference of 1 year. This suggests that the \"typical\" customer firm might be marginally older.\nAge Range (Min and Max): The minimum and maximum ages are quite similar for both groups, ranging from around 9-10 years to 47.5-49 years. This indicates that both younger and older firms can be found in both customer and non-customer categories.\nInterquartile Range (25% and 75%):\nFor non-customers, the interquartile range (IQR) is 31.25 - 21.0 = 10.25 years.\nFor customers, the IQR is 32.50 - 20.5 = 12.0 years. The slightly larger IQR for customers suggests a bit more variability in the middle 50% of their age distribution.\nStandard Deviation: The standard deviation of age is also slightly higher for customers (7.81 years) compared to non-customers (6.95 years), indicating a slightly wider spread in the ages of customer firms overall.\nPotential Implications for the Marketing Claim:\n\nThe fact that Blueprinty customers are, on average, slightly older might be a factor to consider. Older firms might have more established patenting processes or be in industries with a higher propensity to patent. However, the difference in average age is less than a year, so it's unlikely to be the sole driver of any observed differences in patent numbers.\nThe distributions of age for both groups seem relatively similar, with a central tendency in the mid-twenties.\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nThe likelihood function for a single observation $Y$ from a Poisson distribution with parameter $\\lambda$ is:\n\n$$L(\\lambda | Y) = f(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}$$\n\nThe log-likelihood function is:\n\n$$\\ell(\\lambda | Y) = \\ln(L(\\lambda | Y)) = -\\lambda + Y \\ln(\\lambda) - \\ln(Y!)$$\n\n\n\n```{python}\nimport numpy as np\nfrom math import factorial\nimport pandas as pd\ndata = pd.read_csv('blueprinty.csv')\n\n\ndef poisson_loglikelihood(lambd, Y):\n  if lambd <= 0 or not Y == int(Y) or Y < 0:\n    return -np.inf\n  return -lambd + Y * np.log(lambd) - np.log(factorial(Y))\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom math import factorial\n\n# Use the mean number of patents from the entire dataset as Y\nmean_patents_overall = data['patents'].mean()\nY_plot = int(round(mean_patents_overall))\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 2 * mean_patents_overall, 200)\n\n# Calculate log-likelihood values\nloglikelihoods = [poisson_loglikelihood(lambd, Y_plot) for lambd in lambda_values]\n\n# Plot lambda vs. log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, loglikelihoods)\nplt.title(f'Log-Likelihood vs. Lambda (Y = {Y_plot})')\nplt.xlabel('Lambda (λ)')\nplt.ylabel('Log-Likelihood ℓ(λ|Y)')\nplt.grid(True)\nplt.show()\n```\n\n\n\n#### Interpretation of the Log-Likelihood Plot:\n\nThe plot shows the log-likelihood of observing Y=4 (the rounded mean number of patents in our dataset) for different values of the Poisson parameter λ. The curve exhibits a clear peak. This peak represents the value of λ that maximizes the log-likelihood (and thus the likelihood) of observing our data. In this case, the log-likelihood appears to be maximized when λ is approximately 4. This visual confirms that the Maximum Likelihood Estimate (MLE) for λ, given a single observation equal to the sample mean, is the sample mean itself.\n\n\nIf we take the log-likelihood function for a sample of $n$ i.i.d. Poisson observations $y_1, y_2, ..., y_n$:\n\n$$\\ell(\\lambda | y_1, ..., y_n) = -n\\lambda + \\left(\\sum_{i=1}^{n} y_i\\right) \\ln(\\lambda) - \\sum_{i=1}^{n} \\ln(y_i!)$$\n\nTo find the MLE for $\\lambda$, we take the first derivative of the log-likelihood with respect to $\\lambda$ and set it to zero:\n\n$$\\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = -n + \\frac{\\sum_{i=1}^{n} y_i}{\\lambda} = 0$$\n\nSolving for $\\lambda$:\n\n$$\\frac{\\sum_{i=1}^{n} y_i}{\\lambda} = n$$\n\n$$\\lambda = \\frac{\\sum_{i=1}^{n} y_i}{n} = \\bar{Y}$$\n\nThus, the Maximum Likelihood Estimate ($\\lambda_{MLE}$) for the Poisson parameter $\\lambda$ is indeed the sample mean ($\\bar{Y}$). This result aligns with the property that the mean of a Poisson distribution is equal to its parameter $\\lambda$.\n\n\nTo find the Maximum Likelihood Estimate (MLE) for the Poisson parameter λ using numerical optimization, we can use the minimize function from the scipy.optimize library in Python. We define the negative of the log-likelihood function (since minimize performs minimization) for our observed number of patents. By providing an initial guess for λ (such as the sample mean) and running the optimization, we can find the value of λ that minimizes the negative log-likelihood, which is equivalent to maximizing the log-likelihood.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom math import factorial\nfrom scipy.optimize import minimize\n\ndata = pd.read_csv('blueprinty.csv')\nY_observed = data['patents'].values  # Get all observed number of patents\n\ndef negative_poisson_loglikelihood(lambd, Y):\n  \"\"\"Calculates the negative log-likelihood for the entire sample.\"\"\"\n  if lambd <= 0:\n    return np.inf\n  log_likelihood = np.sum(-lambd + Y * np.log(lambd) - np.log(np.array([factorial(y) for y in Y])))\n  return -log_likelihood\n\n# Initial guess for lambda (e.g., the sample mean)\ninitial_lambda = np.mean(Y_observed)\n\n# Perform the optimization\nresult = minimize(negative_poisson_loglikelihood, initial_lambda, args=(Y_observed,), method='L-BFGS-B', bounds=[(0.001, None)])\n\n# Extract the MLE estimate for lambda\nmle_lambda = result.x[0]\n\nprint(f\"Maximum Likelihood Estimate (MLE) for lambda: {mle_lambda:.4f}\")\nprint(f\"Sample Mean of Patents: {initial_lambda:.4f}\")\n```\n\nUsing numerical optimization with the minimize function from scipy.optimize in Python, we found the Maximum Likelihood Estimate (MLE) for the Poisson parameter λ to be approximately 3.6847. This value is remarkably close to the sample mean of the number of patents in our dataset, which is also 3.6847. This empirical result reinforces the theoretical finding that the MLE of λ for a Poisson distribution is the sample mean.\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom math import factorial\nfrom scipy.optimize import minimize\nimport statsmodels.api as sm\n\ndata = pd.read_csv('blueprinty.csv')\nY_observed = data['patents'].values\n\n# Prepare the covariate matrix X, ensuring numeric types\ndata['age_squared'] = pd.to_numeric(data['age'], errors='coerce')**2\nX = pd.get_dummies(data, columns=['region'], drop_first=True)\nX['age'] = pd.to_numeric(X['age'], errors='coerce')\nX['age_squared'] = pd.to_numeric(X['age_squared'], errors='coerce')\nX['iscustomer'] = pd.to_numeric(X['iscustomer'], errors='coerce')\nX = X[['age', 'age_squared', 'iscustomer', 'region_Northeast', 'region_Northwest', 'region_South', 'region_Southwest']].values\n# Add a constant term (intercept) to the matrix X\nX = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1).astype(float)\n\ndef negative_poisson_regression_loglikelihood(beta, Y, X):\n  \"\"\"Calculates the negative log-likelihood for Poisson regression.\"\"\"\n  if np.any(np.isnan(beta)):\n    return np.inf\n  log_lambda = np.dot(X, beta)\n  lambda_i = np.exp(log_lambda)\n  log_likelihood = np.sum(-lambda_i + Y * log_lambda - np.log(np.array([factorial(y) for y in Y])))\n  return -log_likelihood\n\n# Initial guess for beta (zeros), ensuring float type\ninitial_beta = np.zeros(X.shape[1], dtype=float)\n\n# Perform the optimization to find MLE\nresult = minimize(negative_poisson_regression_loglikelihood, initial_beta, args=(Y_observed, X), method='BFGS', jac=None, hess=None)\n\n# Extract the MLE estimates for beta\nmle_beta = result.x\n\n# --- Using statsmodels for standard errors ---\npoisson_model = sm.GLM(Y_observed, X, family=sm.families.Poisson()).fit()\nstandard_errors = poisson_model.bse\ncoefficients = poisson_model.params\n\n# Create a table of coefficients and standard errors\nresults_table = pd.DataFrame({\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors\n})\n\nprint(\"\\nTable of Coefficients and Standard Errors (using statsmodels):\\n\", results_table)\n```\n#### Interpretation of Poisson Regression Coefficients:\n\nWe estimated a Poisson regression model to understand the relationship between the number of patents awarded and firm characteristics, including age, age squared, region, and whether the firm is a Blueprinty customer (iscustomer). \n\n#### Key Observations:\n\nIntercept: The intercept is estimated at -0.509.\nAge: The positive coefficient for age (0.149) suggests that, holding other factors constant, older firms tend to have a higher expected number of patents.\nAge Squared: The negative coefficient for age squared (-0.003) indicates a potential non-linear relationship with age, suggesting that the positive effect of age on patents might diminish at higher ages.\nBlueprinty Customer (iscustomer): The positive and statistically significant coefficient for iscustomer (0.208) is of primary interest. To interpret this, we exponentiate the coefficient: exp(0.208)≈1.23. This suggests that, holding other factors constant, Blueprinty customers have an estimated expected number of patents that is about 23% higher than non-customers.\nRegion: The coefficients for the region dummy variables (Northeast, Northwest, South, Southwest) are relative to the baseline region (Midwest). For example, firms in the Northeast have an estimated expected number of patents that is exp(0.029)≈1.03 times that of firms in the Midwest, after controlling for other variables. The significance of these regional effects would need to be assessed based on their standard errors.\nThese results provide evidence that, even after controlling for age and region, Blueprinty customers tend to have a higher expected number of patents compared to non-customers in this dataset.\n\n\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.genmod import families\n\ndata = pd.read_csv('blueprinty.csv')\n\n# Prepare the covariate matrix X\ndata['age_squared'] = data['age']**2\nX = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n# Select the columns for our model\nX_cols = ['age', 'age_squared', 'iscustomer', 'region_Northeast', 'region_Northwest', 'region_South', 'region_Southwest']\nX = X[X_cols].copy() # Use .copy() to avoid SettingWithCopyWarning\n\n# Explicitly convert X columns to float64\nfor col in X.columns:\n    X[col] = pd.to_numeric(X[col], errors='coerce')\n\n# Add a constant term (intercept) to the matrix X\nX = sm.add_constant(X, prepend=True)\n\n# Ensure the constant is also float\nX['const'] = pd.to_numeric(X['const'], errors='coerce')\n\nY_observed = data['patents']\n\n# Try converting X to numpy array with float dtype\nX_array = np.asarray(X, dtype=float)\n\n# Fit the Poisson GLM\ntry:\n    poisson_glm_model = sm.GLM(Y_observed, X_array, family=families.Poisson()).fit()\n    # Print the summary of the model\n    print(poisson_glm_model.summary())\nexcept ValueError as e:\n    print(f\"ValueError during model fitting: {e}\")\n```\n\n#### Interpretation of the results:\n\nThe results of the Poisson regression model indicate that firm age, age squared, and being a Blueprinty customer are statistically significant predictors of the number of patents awarded. The positive coefficient for iscustomer suggests that, even after controlling for age and regional location, Blueprinty customers have a significantly higher expected number of patents (approximately 23% higher) compared to non-customers. The effects of the different regions compared to the Midwest baseline were not found to be statistically significant in this model.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.genmod import families\n\ndata = pd.read_csv('blueprinty.csv')\n\n# Prepare the covariate matrix X\ndata['age_squared'] = data['age']**2\nX = pd.get_dummies(data, columns=['region'], drop_first=True)\nX = X[['age', 'age_squared', 'iscustomer', 'region_Northeast', 'region_Northwest', 'region_South', 'region_Southwest']].copy()\n\n# Explicitly convert X columns to float64\nfor col in X.columns:\n    X[col] = pd.to_numeric(X[col], errors='coerce').astype(float)\n\n# Add a constant term (intercept) to the matrix X\nX = sm.add_constant(X, prepend=True)\nX['const'] = pd.to_numeric(X['const'], errors='coerce').astype(float)\n\nY_observed = data['patents']\n\n# Fit the Poisson GLM\npoisson_glm_model = sm.GLM(Y_observed, X, family=families.Poisson()).fit()\n\n# Print model summary (for inspection)\nprint(\"\\nPoisson GLM Model Summary before predict:\\n\", poisson_glm_model.summary())\n\n# Print columns of X\nprint(\"\\nColumns of X before creating X_0 and X_1:\\n\", X.columns)\n\n# Create X_0: X data with iscustomer = 0 for all observations\nX_0 = X.copy()\nX_0['iscustomer'] = float(0)\nprint(\"\\nX_0 data types before predict:\\n\", X_0.dtypes)\nprint(\"\\nX_0 head before predict:\\n\", X_0.head())\nprint(\"\\nNumPy array dtype of X_0:\\n\", np.asarray(X_0).dtype)\n\n# Create X_1: X data with iscustomer = 1 for all observations\nX_1 = X.copy()\nX_1['iscustomer'] = float(1)\nprint(\"\\nX_1 data types before predict:\\n\", X_1.dtypes)\nprint(\"\\nX_1 head before predict:\\n\", X_1.head())\nprint(\"\\nNumPy array dtype of X_1:\\n\", np.asarray(X_1).dtype)\n\n# Try predicting on the original X\ntry:\n    log_y_pred_original = poisson_glm_model.predict(X)\n    print(\"\\nPrediction on original X successful.\")\nexcept ValueError as e:\n    print(f\"\\nValueError during prediction on original X: {e}\")\n\n# Predict the log of the expected number of patents for X_0 and X_1\ntry:\n    log_y_pred_0 = poisson_glm_model.predict(X_0)\n    log_y_pred_1 = poisson_glm_model.predict(X_1)\n\n    # The predicted number of patents is exp(log_y_pred)\n    y_pred_0 = np.exp(log_y_pred_0)\n    y_pred_1 = np.exp(log_y_pred_1)\n\n    # Calculate the difference in predicted number of patents\n    difference = y_pred_1 - y_pred_0\n\n    # Calculate the average difference\n    average_difference = np.mean(difference)\n\n    print(f\"\\nAverage predicted number of patents (if all were customers): {np.mean(y_pred_1):.2f}\")\n    print(f\"Average predicted number of patents (if all were non-customers): {np.mean(y_pred_0):.2f}\")\n    print(f\"Average difference in predicted number of patents due to Blueprinty: {average_difference:.2f}\")\n\nexcept ValueError as e:\n    print(f\"ValueError during prediction: {e}\")\n```\n#### Interpretation:\n\nBased on our analysis, the average predicted number of patents over five years for firms in our dataset, if they were all Blueprinty customers, is approximately 83.10. In contrast, the average predicted number of patents if none of them were customers is about 35.49. The average difference between these two scenarios is 47.61 patents. This suggests a substantial positive effect of using Blueprinty's software on patent success, as our model predicts that, on average, firms using the software are expected to be awarded approximately 47.61 more patents over five years compared to non-users, while holding age and regional location constant at their observed levels.\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n####  EDA\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the Airbnb dataset\nairbnb_data = pd.read_csv('airbnb.csv') \n\n# Basic information about the dataset\nprint(\"Airbnb Data Info:\")\nairbnb_data.info()\n\n# Summary statistics for numerical variables\nprint(\"\\nAirbnb Data Summary Statistics:\")\nprint(airbnb_data.describe())\n\n# Look at the distribution of the proxy for bookings (number_of_reviews)\nplt.figure(figsize=(10, 6))\nsns.histplot(airbnb_data['number_of_reviews'], bins=50, kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.show()\n\n# Explore the relationship between room_type and number_of_reviews\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='room_type', y='number_of_reviews', data=airbnb_data)\nplt.title('Number of Reviews by Room Type')\nplt.xlabel('Room Type')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n# Explore the relationship between price and number_of_reviews (might be non-linear)\nplt.figure(figsize=(10, 6))\nplt.scatter(airbnb_data['price'], airbnb_data['number_of_reviews'], alpha=0.3)\nplt.title('Number of Reviews vs. Price')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.xlim(0, 1000) # Limit price for better visualization\nplt.ylim(0, 200)  # Limit reviews for better visualization\nplt.show()\n\n# Explore the distribution of review scores\nreview_scores_cols = ['review_scores_cleanliness', 'review_scores_location', 'review_scores_value']\nairbnb_data[review_scores_cols].hist(figsize=(12, 4), bins=10)\nplt.suptitle('Distribution of Review Scores', y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Explore the 'instant_bookable' variable\nprint(\"\\nValue counts for instant_bookable:\")\nprint(airbnb_data['instant_bookable'].value_counts())\n```\n#### Handling Missing Values\n```{python}\nimport pandas as pd\nimport numpy as np\n\n# Load the Airbnb dataset\nairbnb_data = pd.read_csv('airbnb.csv')\n\n# Convert 'last_scraped' and 'host_since' to datetime objects\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Calculate 'days' the unit has been listed\nairbnb_data['days'] = (airbnb_data['last_scraped'] - airbnb_data['host_since']).dt.days\n\n# Identify relevant columns for modeling\nrelevant_cols = ['number_of_reviews', 'room_type', 'bathrooms', 'bedrooms', 'price',\n                 'review_scores_cleanliness', 'review_scores_location',\n                 'review_scores_value', 'instant_bookable', 'days']\n\n# Check for missing values in the relevant columns\nmissing_values = airbnb_data[relevant_cols].isnull().sum()\nprint(\"Missing values in relevant columns:\\n\", missing_values)\n\n# Handle missing values\n# Option 1: Drop rows with any missing values in the relevant columns\nairbnb_data_cleaned = airbnb_data[relevant_cols].dropna()\nprint(\"\\nShape of data before dropping missing values:\", airbnb_data[relevant_cols].shape)\nprint(\"Shape of data after dropping missing values:\", airbnb_data_cleaned.shape)\n\n# Option 2: Impute missing review scores (e.g., with the mean or median)\n# You might choose this if you want to retain more data, but for simplicity,\n# we'll proceed with dropping rows for now.\n\n# Verify that there are no missing values in the cleaned data\nprint(\"\\nMissing values in cleaned data:\\n\", airbnb_data_cleaned.isnull().sum())\n```\n\n\n#### Poisson Regression Model\n```{python}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.genmod import families\n\n# Load the Airbnb dataset\nairbnb_data = pd.read_csv('airbnb.csv')\n\n# Convert 'last_scraped' and 'host_since' to datetime objects\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Calculate 'days' the unit has been listed\nairbnb_data['days'] = (airbnb_data['last_scraped'] - airbnb_data['host_since']).dt.days\n\n# Define relevant columns and drop rows with missing values\nrelevant_cols = ['number_of_reviews', 'room_type', 'bathrooms', 'bedrooms', 'price',\n                 'review_scores_cleanliness', 'review_scores_location',\n                 'review_scores_value', 'instant_bookable', 'days']\nairbnb_data_cleaned = airbnb_data[relevant_cols].dropna().copy()\n\n# Convert categorical variables to dummy variables\nairbnb_data_cleaned = pd.get_dummies(airbnb_data_cleaned, columns=['room_type', 'instant_bookable'], drop_first=True)\n\n# Define the dependent variable (number_of_reviews) and independent variables\ny = airbnb_data_cleaned['number_of_reviews']\nX = airbnb_data_cleaned.drop('number_of_reviews', axis=1).copy() # Use .copy()\n\n# Explicitly convert all columns in X to float64\nfor col in X.columns:\n    X[col] = pd.to_numeric(X[col], errors='coerce').astype(float)\n\n# Add a constant to the independent variables for the intercept\nX = sm.add_constant(X, prepend=True)\nX['const'] = pd.to_numeric(X['const'], errors='coerce').astype(float)\n\n# Check the data types of X before fitting\nprint(\"Data types of X before fitting GLM:\\n\", X.dtypes)\nprint(\"\\nHead of X before fitting GLM:\\n\", X.head())\nprint(\"\\nData type of y before fitting GLM:\\n\", y.dtype)\nprint(\"\\nHead of y before fitting GLM:\\n\", y.head())\n\n# Fit the Poisson regression model\ntry:\n    poisson_model = sm.GLM(y, X, family=families.Poisson()).fit()\n    # Print the model summary\n    print(poisson_model.summary())\nexcept ValueError as e:\n    print(f\"ValueError during model fitting: {e}\")\n```\n\n#### Interpretation of the Poisson Regression Coefficients\n\nThe coefficients in a Poisson regression model (with a log link function) represent the change in the log of the expected outcome (number of reviews) for a one-unit increase in the predictor variable, holding other predictors constant. To interpret the effect on the expected number of reviews itself, we need to exponentiate the coefficients. exp(β) gives the multiplicative factor by which the expected number of reviews changes.\n\nHere's a breakdown of the coefficients:\n\nconst (-0.6881): This is the intercept. When all other predictors are zero, the log of the expected number of reviews is -0.6881. exp(−0.6881)≈0.50. This baseline might not be practically meaningful as some predictors (like review scores) cannot be zero.\n\nbathrooms (0.0817): For each additional bathroom, the log of the expected number of reviews increases by 0.0817. exp(0.0817)≈1.085. So, holding other factors constant, each additional bathroom is associated with an approximately 8.5% increase in the expected number of reviews. This effect is statistically significant (p < 0.001).\n\nbedrooms (0.0484): For each additional bedroom, the log of the expected number of reviews increases by 0.0484. exp(0.0484)≈1.050. Each additional bedroom is associated with an approximately 5.0% increase in the expected number of reviews (significant, p < 0.001).\n\nprice (-0.0007): For each one-dollar increase in price, the log of the expected number of reviews decreases by 0.0007. exp(−0.0007)≈0.9993. A one-dollar increase in price is associated with a very small (0.07%) decrease in the expected number of reviews (significant, p < 0.001).\n\nreview_scores_cleanliness (0.3284): For each one-point increase in the cleanliness score (on a 1-10 scale), the log of the expected number of reviews increases by 0.3284. exp(0.3284)≈1.389. A one-point increase in cleanliness score is associated with an approximately 38.9% increase in the expected number of reviews (significant, p < 0.001).\n\nreview_scores_location (0.1528): For each one-point increase in the location score, the log of the expected number of reviews increases by 0.1528. exp(0.1528)≈1.165. A one-point increase in location score is associated with an approximately 16.5% increase in the expected number of reviews (significant, p < 0.001).\n\nreview_scores_value (0.2192): For each one-point increase in the value score, the log of the expected number of reviews increases by 0.2192. exp(0.2192)≈1.245. A one-point increase in value score is associated with an approximately 24.5% increase in the expected number of reviews (significant, p < 0.001).\n\ndays (0.0004): For each additional day the unit has been listed, the log of the expected number of reviews increases by 0.0004. exp(0.0004)≈1.0004. Each additional day listed is associated with a very small (0.04%) increase in the expected number of reviews (significant, p < 0.001).\n\nroom_type_Private room (-0.2640): Compared to an entire home/apartment (the baseline), private rooms have a lower expected number of reviews. exp(−0.2640)≈0.768. Private rooms are associated with approximately 23.2% fewer expected reviews (significant, p < 0.001).\n\nroom_type_Shared room (-1.1152): Compared to an entire home/apartment, shared rooms have a much lower expected number of reviews. exp(−1.1152)≈0.328. Shared rooms are associated with approximately 67.2% fewer expected reviews (significant, p < 0.001).\n\ninstant_bookable_t (0.4591): Listings that are instantly bookable have a higher expected number of reviews. exp(0.4591)≈1.582. Instantly bookable listings are associated with approximately 58.2% more expected reviews (significant, p < 0.001).\n\n#### Summary:\n\nThe Poisson regression model reveals several factors associated with the number of reviews (our proxy for bookings) of Airbnb listings in New York City. Listings with more bathrooms and bedrooms tend to have more reviews. Higher prices are associated with a very slight decrease in reviews. Review scores, particularly for cleanliness, location, and value, show a strong positive association with the number of reviews. The longer a listing has been active (more days), the slightly more reviews it tends to have. Compared to entire homes/apartments, private and especially shared rooms tend to have significantly fewer reviews. Finally, listings that are instantly bookable have a substantially higher number of reviews. All these effects are statistically significant at the p < 0.001 level.\n\nThis analysis provides insights into what factors might drive the popularity (as measured by the number of reviews) of Airbnb listings in NYC.\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.5","theme":["cosmo","brand"],"title":"Poisson Regression Examples","author":"Sumedh Hambarde","date":"today","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}