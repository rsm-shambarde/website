---
title: "Poisson Regression Examples"
author: "Sumedh Hambarde"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty hus collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Replace 'your_data.csv' with the actual path to your data file
data = pd.read_csv('blueprinty.csv')

# It's always a good idea to get a quick look at the data
print(data.head())
print(data.info())
print(data.describe())
# The column indicating Blueprinty usage is 'iscustomer' (0 for no, 1 for yes)
customer_users = data[data['iscustomer'] == 1]
non_customers = data[data['iscustomer'] == 0]

# The column with the number of patents is 'patents'
patents_column = 'patents'

# Create histograms
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.histplot(customer_users[patents_column], kde=True)
plt.title('Number of Patents (Blueprinty Customers)')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
sns.histplot(non_customers[patents_column], kde=True)
plt.title('Number of Patents (Non-Customers)')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Calculate means
mean_patents_customer = customer_users[patents_column].mean()
mean_patents_non_customer = non_customers[patents_column].mean()

print(f"\nMean number of patents (Blueprinty customers): {mean_patents_customer:.2f}")
print(f"Mean number of patents (Non-customers): {mean_patents_non_customer:.2f}")
```


The histograms reveal that the distribution of patents awarded is right-skewed for both Blueprinty customers and non-customers, but the distribution for customers appears shifted towards a higher number of patents, with a peak around 4-5 compared to 2-3 for non-customers; this visual difference is supported by the mean number of patents, which is 4.13 for customers and 3.47 for non-customers, suggesting that, on average, Blueprinty customers have a higher number of patents, although further statistical modeling is needed to confirm the significance and account for other factors.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import io
data = pd.read_csv('blueprinty.csv')

# Assuming you have already read your data into a pandas DataFrame named 'data'
# If not, uncomment and replace 'your_data.csv' with the actual path
# data = pd.read_csv('your_data.csv')

# --- Comparing Regions by Customer Status ---
region_customer_table = pd.crosstab(data['region'], data['iscustomer'])
print("Region vs. Customer Status Contingency Table:\n", region_customer_table.to_string())

# Create a stacked bar chart (this will show a plot)
region_customer_table.plot(kind='bar', stacked=True, figsize=(8, 6))
plt.title('Regional Distribution by Customer Status')
plt.xlabel('Region')
plt.ylabel('Number of Firms')
plt.xticks(rotation=0)
plt.legend(title='Is Customer', labels=['No', 'Yes'])
plt.show()
plt.close() # Close the plot to avoid it showing up again later

# --- Comparing Ages by Customer Status ---
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(data[data['iscustomer'] == 1]['age'], kde=True)
plt.title('Age of Firms (Blueprinty Customers)')
plt.xlabel('Age (Years)')
plt.ylabel('Frequency')
plt.show()
plt.close()

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 2)
sns.histplot(data[data['iscustomer'] == 0]['age'], kde=True)
plt.title('Age of Firms (Non-Customers)')
plt.xlabel('Age (Years)')
plt.ylabel('Frequency')
plt.show()
plt.close()

plt.figure(figsize=(8, 6))
sns.boxplot(x='iscustomer', y='age', data=data)
plt.title('Age of Firms by Customer Status')
plt.xlabel('Is Customer')
plt.ylabel('Age (Years)')
plt.xticks([0, 1], ['No', 'Yes'])
plt.show()
plt.close()

age_summary = data.groupby('iscustomer')['age'].describe()
print("\nAge Summary Statistics:\n", age_summary.to_string())

# --- Comparing Patents (as done previously) ---
customer_users = data[data['iscustomer'] == 1]
non_customers = data[data['iscustomer'] == 0]
patents_column = 'patents'

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(customer_users[patents_column], kde=True)
plt.title('Number of Patents (Blueprinty Customers)')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.show()
plt.close()

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 2)
sns.histplot(non_customers[patents_column], kde=True)
plt.title('Number of Patents (Non-Customers)')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.show()
plt.close()

mean_patents_customer = customer_users[patents_column].mean()
mean_patents_non_customer = non_customers[patents_column].mean()

print(f"\nMean number of patents (Blueprinty customers): {mean_patents_customer:.2f}")
print(f"Mean number of patents (Non-customers): {mean_patents_non_customer:.2f}")
```

#### Observations from the Region vs. Customer Status Contingency Table:

Total Firms per Region:

Midwest: 187 (non-customer) + 37 (customer) = 224
Northeast: 273 (non-customer) + 328 (customer) = 601
Northwest: 158 (non-customer) + 29 (customer) = 187
South: 156 (non-customer) + 35 (customer) = 191
Southwest: 245 (non-customer) + 52 (customer) = 297
Distribution of Customers:

The Northeast region has the highest number of customers (328), significantly more than any other region.
The Southwest region has the second-highest number of customers (52).
The Midwest and South regions have a similar, lower number of customers (37 and 35, respectively).
The Northwest has the fewest customers (29).
Distribution of Non-Customers:

The Northeast also has the highest number of non-customers (273).
The Southwest has the second-highest number of non-customers (245).
The Midwest has a substantial number of non-customers (187).
The South and Northwest have a lower number of non-customers (156 and 158, respectively).
Proportion of Customers within each Region: To get a better sense of whether customers are disproportionately located in certain regions, we can calculate the percentage of customers in each region:

Midwest: (37 / 224) * 100% ≈ 16.5%
Northeast: (328 / 601) * 100% ≈ 54.6%
Northwest: (29 / 187) * 100% ≈ 15.5%
South: (35 / 191) * 100% ≈ 18.3%
Southwest: (52 / 297) * 100% ≈ 17.5%
Observations on Proportions:

The proportion of Blueprinty customers is strikingly higher in the Northeast region (around 54.6%) compared to all other regions, where the proportion ranges from approximately 15.5% to 18.3%.
Potential Implications for the Marketing Claim:

The strong concentration of Blueprinty customers in the Northeast region raises a crucial point. If the Northeast region is inherently more innovative or has a higher propensity for patent applications due to other factors (e.g., industry concentration, research institutions), then the higher number of patents among Blueprinty customers might be partly or even largely attributable to their regional location rather than solely due to the software.
To properly assess the effect of Blueprinty, it will be important to control for the "region" variable in any statistical models you build.

#### Observations on Firm Age by Customer Status:

Mean Age: The mean age of Blueprinty customers (26.90 years) is slightly higher than the mean age of non-customers (26.10 years). The difference is about 0.8 years. While this difference exists, it doesn't appear to be dramatically large.
Median Age (50%): The median age of customers (26.5 years) is also slightly higher than that of non-customers (25.5 years), with a difference of 1 year. This suggests that the "typical" customer firm might be marginally older.
Age Range (Min and Max): The minimum and maximum ages are quite similar for both groups, ranging from around 9-10 years to 47.5-49 years. This indicates that both younger and older firms can be found in both customer and non-customer categories.
Interquartile Range (25% and 75%):
For non-customers, the interquartile range (IQR) is 31.25 - 21.0 = 10.25 years.
For customers, the IQR is 32.50 - 20.5 = 12.0 years. The slightly larger IQR for customers suggests a bit more variability in the middle 50% of their age distribution.
Standard Deviation: The standard deviation of age is also slightly higher for customers (7.81 years) compared to non-customers (6.95 years), indicating a slightly wider spread in the ages of customer firms overall.
Potential Implications for the Marketing Claim:

The fact that Blueprinty customers are, on average, slightly older might be a factor to consider. Older firms might have more established patenting processes or be in industries with a higher propensity to patent. However, the difference in average age is less than a year, so it's unlikely to be the sole driver of any observed differences in patent numbers.
The distributions of age for both groups seem relatively similar, with a central tendency in the mid-twenties.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

The likelihood function for a single observation $Y$ from a Poisson distribution with parameter $\lambda$ is:

$$L(\lambda | Y) = f(Y|\lambda) = \frac{e^{-\lambda} \lambda^Y}{Y!}$$

The log-likelihood function is:

$$\ell(\lambda | Y) = \ln(L(\lambda | Y)) = -\lambda + Y \ln(\lambda) - \ln(Y!)$$



```{python}
import numpy as np
from math import factorial
import pandas as pd
data = pd.read_csv('blueprinty.csv')


def poisson_loglikelihood(lambd, Y):
  if lambd <= 0 or not Y == int(Y) or Y < 0:
    return -np.inf
  return -lambd + Y * np.log(lambd) - np.log(factorial(Y))

import numpy as np
import matplotlib.pyplot as plt
from math import factorial

# Use the mean number of patents from the entire dataset as Y
mean_patents_overall = data['patents'].mean()
Y_plot = int(round(mean_patents_overall))

# Define a range of lambda values
lambda_values = np.linspace(0.1, 2 * mean_patents_overall, 200)

# Calculate log-likelihood values
loglikelihoods = [poisson_loglikelihood(lambd, Y_plot) for lambd in lambda_values]

# Plot lambda vs. log-likelihood
plt.figure(figsize=(10, 6))
plt.plot(lambda_values, loglikelihoods)
plt.title(f'Log-Likelihood vs. Lambda (Y = {Y_plot})')
plt.xlabel('Lambda (λ)')
plt.ylabel('Log-Likelihood ℓ(λ|Y)')
plt.grid(True)
plt.show()
```



#### Interpretation of the Log-Likelihood Plot:

The plot shows the log-likelihood of observing Y=4 (the rounded mean number of patents in our dataset) for different values of the Poisson parameter λ. The curve exhibits a clear peak. This peak represents the value of λ that maximizes the log-likelihood (and thus the likelihood) of observing our data. In this case, the log-likelihood appears to be maximized when λ is approximately 4. This visual confirms that the Maximum Likelihood Estimate (MLE) for λ, given a single observation equal to the sample mean, is the sample mean itself.


If we take the log-likelihood function for a sample of $n$ i.i.d. Poisson observations $y_1, y_2, ..., y_n$:

$$\ell(\lambda | y_1, ..., y_n) = -n\lambda + \left(\sum_{i=1}^{n} y_i\right) \ln(\lambda) - \sum_{i=1}^{n} \ln(y_i!)$$

To find the MLE for $\lambda$, we take the first derivative of the log-likelihood with respect to $\lambda$ and set it to zero:

$$\frac{\partial \ell(\lambda)}{\partial \lambda} = -n + \frac{\sum_{i=1}^{n} y_i}{\lambda} = 0$$

Solving for $\lambda$:

$$\frac{\sum_{i=1}^{n} y_i}{\lambda} = n$$

$$\lambda = \frac{\sum_{i=1}^{n} y_i}{n} = \bar{Y}$$

Thus, the Maximum Likelihood Estimate ($\lambda_{MLE}$) for the Poisson parameter $\lambda$ is indeed the sample mean ($\bar{Y}$). This result aligns with the property that the mean of a Poisson distribution is equal to its parameter $\lambda$.


To find the Maximum Likelihood Estimate (MLE) for the Poisson parameter λ using numerical optimization, we can use the minimize function from the scipy.optimize library in Python. We define the negative of the log-likelihood function (since minimize performs minimization) for our observed number of patents. By providing an initial guess for λ (such as the sample mean) and running the optimization, we can find the value of λ that minimizes the negative log-likelihood, which is equivalent to maximizing the log-likelihood.

```{python}
import pandas as pd
import numpy as np
from math import factorial
from scipy.optimize import minimize

data = pd.read_csv('blueprinty.csv')
Y_observed = data['patents'].values  # Get all observed number of patents

def negative_poisson_loglikelihood(lambd, Y):
  """Calculates the negative log-likelihood for the entire sample."""
  if lambd <= 0:
    return np.inf
  log_likelihood = np.sum(-lambd + Y * np.log(lambd) - np.log(np.array([factorial(y) for y in Y])))
  return -log_likelihood

# Initial guess for lambda (e.g., the sample mean)
initial_lambda = np.mean(Y_observed)

# Perform the optimization
result = minimize(negative_poisson_loglikelihood, initial_lambda, args=(Y_observed,), method='L-BFGS-B', bounds=[(0.001, None)])

# Extract the MLE estimate for lambda
mle_lambda = result.x[0]

print(f"Maximum Likelihood Estimate (MLE) for lambda: {mle_lambda:.4f}")
print(f"Sample Mean of Patents: {initial_lambda:.4f}")
```

Using numerical optimization with the minimize function from scipy.optimize in Python, we found the Maximum Likelihood Estimate (MLE) for the Poisson parameter λ to be approximately 3.6847. This value is remarkably close to the sample mean of the number of patents in our dataset, which is also 3.6847. This empirical result reinforces the theoretical finding that the MLE of λ for a Poisson distribution is the sample mean.

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

```{python}
import pandas as pd
import numpy as np
from math import factorial
from scipy.optimize import minimize
import statsmodels.api as sm

data = pd.read_csv('blueprinty.csv')
Y_observed = data['patents'].values

# Prepare the covariate matrix X, ensuring numeric types
data['age_squared'] = pd.to_numeric(data['age'], errors='coerce')**2
X = pd.get_dummies(data, columns=['region'], drop_first=True)
X['age'] = pd.to_numeric(X['age'], errors='coerce')
X['age_squared'] = pd.to_numeric(X['age_squared'], errors='coerce')
X['iscustomer'] = pd.to_numeric(X['iscustomer'], errors='coerce')
X = X[['age', 'age_squared', 'iscustomer', 'region_Northeast', 'region_Northwest', 'region_South', 'region_Southwest']].values
# Add a constant term (intercept) to the matrix X
X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1).astype(float)

def negative_poisson_regression_loglikelihood(beta, Y, X):
  """Calculates the negative log-likelihood for Poisson regression."""
  if np.any(np.isnan(beta)):
    return np.inf
  log_lambda = np.dot(X, beta)
  lambda_i = np.exp(log_lambda)
  log_likelihood = np.sum(-lambda_i + Y * log_lambda - np.log(np.array([factorial(y) for y in Y])))
  return -log_likelihood

# Initial guess for beta (zeros), ensuring float type
initial_beta = np.zeros(X.shape[1], dtype=float)

# Perform the optimization to find MLE
result = minimize(negative_poisson_regression_loglikelihood, initial_beta, args=(Y_observed, X), method='BFGS', jac=None, hess=None)

# Extract the MLE estimates for beta
mle_beta = result.x

# --- Using statsmodels for standard errors ---
poisson_model = sm.GLM(Y_observed, X, family=sm.families.Poisson()).fit()
standard_errors = poisson_model.bse
coefficients = poisson_model.params

# Create a table of coefficients and standard errors
results_table = pd.DataFrame({
    'Coefficient': coefficients,
    'Standard Error': standard_errors
})

print("\nTable of Coefficients and Standard Errors (using statsmodels):\n", results_table)
```
#### Interpretation of Poisson Regression Coefficients:

We estimated a Poisson regression model to understand the relationship between the number of patents awarded and firm characteristics, including age, age squared, region, and whether the firm is a Blueprinty customer (iscustomer). 

#### Key Observations:

Intercept: The intercept is estimated at -0.509.
Age: The positive coefficient for age (0.149) suggests that, holding other factors constant, older firms tend to have a higher expected number of patents.
Age Squared: The negative coefficient for age squared (-0.003) indicates a potential non-linear relationship with age, suggesting that the positive effect of age on patents might diminish at higher ages.
Blueprinty Customer (iscustomer): The positive and statistically significant coefficient for iscustomer (0.208) is of primary interest. To interpret this, we exponentiate the coefficient: exp(0.208)≈1.23. This suggests that, holding other factors constant, Blueprinty customers have an estimated expected number of patents that is about 23% higher than non-customers.
Region: The coefficients for the region dummy variables (Northeast, Northwest, South, Southwest) are relative to the baseline region (Midwest). For example, firms in the Northeast have an estimated expected number of patents that is exp(0.029)≈1.03 times that of firms in the Midwest, after controlling for other variables. The significance of these regional effects would need to be assessed based on their standard errors.
These results provide evidence that, even after controlling for age and region, Blueprinty customers tend to have a higher expected number of patents compared to non-customers in this dataset.



```{python}
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.genmod import families

data = pd.read_csv('blueprinty.csv')

# Prepare the covariate matrix X
data['age_squared'] = data['age']**2
X = pd.get_dummies(data, columns=['region'], drop_first=True)

# Select the columns for our model
X_cols = ['age', 'age_squared', 'iscustomer', 'region_Northeast', 'region_Northwest', 'region_South', 'region_Southwest']
X = X[X_cols].copy() # Use .copy() to avoid SettingWithCopyWarning

# Explicitly convert X columns to float64
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')

# Add a constant term (intercept) to the matrix X
X = sm.add_constant(X, prepend=True)

# Ensure the constant is also float
X['const'] = pd.to_numeric(X['const'], errors='coerce')

Y_observed = data['patents']

# Try converting X to numpy array with float dtype
X_array = np.asarray(X, dtype=float)

# Fit the Poisson GLM
try:
    poisson_glm_model = sm.GLM(Y_observed, X_array, family=families.Poisson()).fit()
    # Print the summary of the model
    print(poisson_glm_model.summary())
except ValueError as e:
    print(f"ValueError during model fitting: {e}")
```

#### Interpretation of the results:

The results of the Poisson regression model indicate that firm age, age squared, and being a Blueprinty customer are statistically significant predictors of the number of patents awarded. The positive coefficient for iscustomer suggests that, even after controlling for age and regional location, Blueprinty customers have a significantly higher expected number of patents (approximately 23% higher) compared to non-customers. The effects of the different regions compared to the Midwest baseline were not found to be statistically significant in this model.

```{python}
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.genmod import families

data = pd.read_csv('blueprinty.csv')

# Prepare the covariate matrix X
data['age_squared'] = data['age']**2
X = pd.get_dummies(data, columns=['region'], drop_first=True)
X = X[['age', 'age_squared', 'iscustomer', 'region_Northeast', 'region_Northwest', 'region_South', 'region_Southwest']].copy()

# Explicitly convert X columns to float64
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce').astype(float)

# Add a constant term (intercept) to the matrix X
X = sm.add_constant(X, prepend=True)
X['const'] = pd.to_numeric(X['const'], errors='coerce').astype(float)

Y_observed = data['patents']

# Fit the Poisson GLM
poisson_glm_model = sm.GLM(Y_observed, X, family=families.Poisson()).fit()

# Print model summary (for inspection)
print("\nPoisson GLM Model Summary before predict:\n", poisson_glm_model.summary())

# Print columns of X
print("\nColumns of X before creating X_0 and X_1:\n", X.columns)

# Create X_0: X data with iscustomer = 0 for all observations
X_0 = X.copy()
X_0['iscustomer'] = float(0)
print("\nX_0 data types before predict:\n", X_0.dtypes)
print("\nX_0 head before predict:\n", X_0.head())
print("\nNumPy array dtype of X_0:\n", np.asarray(X_0).dtype)

# Create X_1: X data with iscustomer = 1 for all observations
X_1 = X.copy()
X_1['iscustomer'] = float(1)
print("\nX_1 data types before predict:\n", X_1.dtypes)
print("\nX_1 head before predict:\n", X_1.head())
print("\nNumPy array dtype of X_1:\n", np.asarray(X_1).dtype)

# Try predicting on the original X
try:
    log_y_pred_original = poisson_glm_model.predict(X)
    print("\nPrediction on original X successful.")
except ValueError as e:
    print(f"\nValueError during prediction on original X: {e}")

# Predict the log of the expected number of patents for X_0 and X_1
try:
    log_y_pred_0 = poisson_glm_model.predict(X_0)
    log_y_pred_1 = poisson_glm_model.predict(X_1)

    # The predicted number of patents is exp(log_y_pred)
    y_pred_0 = np.exp(log_y_pred_0)
    y_pred_1 = np.exp(log_y_pred_1)

    # Calculate the difference in predicted number of patents
    difference = y_pred_1 - y_pred_0

    # Calculate the average difference
    average_difference = np.mean(difference)

    print(f"\nAverage predicted number of patents (if all were customers): {np.mean(y_pred_1):.2f}")
    print(f"Average predicted number of patents (if all were non-customers): {np.mean(y_pred_0):.2f}")
    print(f"Average difference in predicted number of patents due to Blueprinty: {average_difference:.2f}")

except ValueError as e:
    print(f"ValueError during prediction: {e}")
```
#### Interpretation:

Based on our analysis, the average predicted number of patents over five years for firms in our dataset, if they were all Blueprinty customers, is approximately 83.10. In contrast, the average predicted number of patents if none of them were customers is about 35.49. The average difference between these two scenarios is 47.61 patents. This suggests a substantial positive effect of using Blueprinty's software on patent success, as our model predicts that, on average, firms using the software are expected to be awarded approximately 47.61 more patents over five years compared to non-users, while holding age and regional location constant at their observed levels.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::
####  EDA

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the Airbnb dataset
airbnb_data = pd.read_csv('airbnb.csv') 

# Basic information about the dataset
print("Airbnb Data Info:")
airbnb_data.info()

# Summary statistics for numerical variables
print("\nAirbnb Data Summary Statistics:")
print(airbnb_data.describe())

# Look at the distribution of the proxy for bookings (number_of_reviews)
plt.figure(figsize=(10, 6))
sns.histplot(airbnb_data['number_of_reviews'], bins=50, kde=True)
plt.title('Distribution of Number of Reviews')
plt.xlabel('Number of Reviews')
plt.ylabel('Frequency')
plt.show()

# Explore the relationship between room_type and number_of_reviews
plt.figure(figsize=(8, 6))
sns.boxplot(x='room_type', y='number_of_reviews', data=airbnb_data)
plt.title('Number of Reviews by Room Type')
plt.xlabel('Room Type')
plt.ylabel('Number of Reviews')
plt.show()

# Explore the relationship between price and number_of_reviews (might be non-linear)
plt.figure(figsize=(10, 6))
plt.scatter(airbnb_data['price'], airbnb_data['number_of_reviews'], alpha=0.3)
plt.title('Number of Reviews vs. Price')
plt.xlabel('Price')
plt.ylabel('Number of Reviews')
plt.xlim(0, 1000) # Limit price for better visualization
plt.ylim(0, 200)  # Limit reviews for better visualization
plt.show()

# Explore the distribution of review scores
review_scores_cols = ['review_scores_cleanliness', 'review_scores_location', 'review_scores_value']
airbnb_data[review_scores_cols].hist(figsize=(12, 4), bins=10)
plt.suptitle('Distribution of Review Scores', y=1.02)
plt.tight_layout()
plt.show()

# Explore the 'instant_bookable' variable
print("\nValue counts for instant_bookable:")
print(airbnb_data['instant_bookable'].value_counts())
```
#### Handling Missing Values
```{python}
import pandas as pd
import numpy as np

# Load the Airbnb dataset
airbnb_data = pd.read_csv('airbnb.csv')

# Convert 'last_scraped' and 'host_since' to datetime objects
airbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])
airbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])

# Calculate 'days' the unit has been listed
airbnb_data['days'] = (airbnb_data['last_scraped'] - airbnb_data['host_since']).dt.days

# Identify relevant columns for modeling
relevant_cols = ['number_of_reviews', 'room_type', 'bathrooms', 'bedrooms', 'price',
                 'review_scores_cleanliness', 'review_scores_location',
                 'review_scores_value', 'instant_bookable', 'days']

# Check for missing values in the relevant columns
missing_values = airbnb_data[relevant_cols].isnull().sum()
print("Missing values in relevant columns:\n", missing_values)

# Handle missing values
# Option 1: Drop rows with any missing values in the relevant columns
airbnb_data_cleaned = airbnb_data[relevant_cols].dropna()
print("\nShape of data before dropping missing values:", airbnb_data[relevant_cols].shape)
print("Shape of data after dropping missing values:", airbnb_data_cleaned.shape)

# Option 2: Impute missing review scores (e.g., with the mean or median)
# You might choose this if you want to retain more data, but for simplicity,
# we'll proceed with dropping rows for now.

# Verify that there are no missing values in the cleaned data
print("\nMissing values in cleaned data:\n", airbnb_data_cleaned.isnull().sum())
```


#### Poisson Regression Model
```{python}
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.genmod import families

# Load the Airbnb dataset
airbnb_data = pd.read_csv('airbnb.csv')

# Convert 'last_scraped' and 'host_since' to datetime objects
airbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])
airbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])

# Calculate 'days' the unit has been listed
airbnb_data['days'] = (airbnb_data['last_scraped'] - airbnb_data['host_since']).dt.days

# Define relevant columns and drop rows with missing values
relevant_cols = ['number_of_reviews', 'room_type', 'bathrooms', 'bedrooms', 'price',
                 'review_scores_cleanliness', 'review_scores_location',
                 'review_scores_value', 'instant_bookable', 'days']
airbnb_data_cleaned = airbnb_data[relevant_cols].dropna().copy()

# Convert categorical variables to dummy variables
airbnb_data_cleaned = pd.get_dummies(airbnb_data_cleaned, columns=['room_type', 'instant_bookable'], drop_first=True)

# Define the dependent variable (number_of_reviews) and independent variables
y = airbnb_data_cleaned['number_of_reviews']
X = airbnb_data_cleaned.drop('number_of_reviews', axis=1).copy() # Use .copy()

# Explicitly convert all columns in X to float64
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce').astype(float)

# Add a constant to the independent variables for the intercept
X = sm.add_constant(X, prepend=True)
X['const'] = pd.to_numeric(X['const'], errors='coerce').astype(float)

# Check the data types of X before fitting
print("Data types of X before fitting GLM:\n", X.dtypes)
print("\nHead of X before fitting GLM:\n", X.head())
print("\nData type of y before fitting GLM:\n", y.dtype)
print("\nHead of y before fitting GLM:\n", y.head())

# Fit the Poisson regression model
try:
    poisson_model = sm.GLM(y, X, family=families.Poisson()).fit()
    # Print the model summary
    print(poisson_model.summary())
except ValueError as e:
    print(f"ValueError during model fitting: {e}")
```

#### Interpretation of the Poisson Regression Coefficients

The coefficients in a Poisson regression model (with a log link function) represent the change in the log of the expected outcome (number of reviews) for a one-unit increase in the predictor variable, holding other predictors constant. To interpret the effect on the expected number of reviews itself, we need to exponentiate the coefficients. exp(β) gives the multiplicative factor by which the expected number of reviews changes.

Here's a breakdown of the coefficients:

const (-0.6881): This is the intercept. When all other predictors are zero, the log of the expected number of reviews is -0.6881. exp(−0.6881)≈0.50. This baseline might not be practically meaningful as some predictors (like review scores) cannot be zero.

bathrooms (0.0817): For each additional bathroom, the log of the expected number of reviews increases by 0.0817. exp(0.0817)≈1.085. So, holding other factors constant, each additional bathroom is associated with an approximately 8.5% increase in the expected number of reviews. This effect is statistically significant (p < 0.001).

bedrooms (0.0484): For each additional bedroom, the log of the expected number of reviews increases by 0.0484. exp(0.0484)≈1.050. Each additional bedroom is associated with an approximately 5.0% increase in the expected number of reviews (significant, p < 0.001).

price (-0.0007): For each one-dollar increase in price, the log of the expected number of reviews decreases by 0.0007. exp(−0.0007)≈0.9993. A one-dollar increase in price is associated with a very small (0.07%) decrease in the expected number of reviews (significant, p < 0.001).

review_scores_cleanliness (0.3284): For each one-point increase in the cleanliness score (on a 1-10 scale), the log of the expected number of reviews increases by 0.3284. exp(0.3284)≈1.389. A one-point increase in cleanliness score is associated with an approximately 38.9% increase in the expected number of reviews (significant, p < 0.001).

review_scores_location (0.1528): For each one-point increase in the location score, the log of the expected number of reviews increases by 0.1528. exp(0.1528)≈1.165. A one-point increase in location score is associated with an approximately 16.5% increase in the expected number of reviews (significant, p < 0.001).

review_scores_value (0.2192): For each one-point increase in the value score, the log of the expected number of reviews increases by 0.2192. exp(0.2192)≈1.245. A one-point increase in value score is associated with an approximately 24.5% increase in the expected number of reviews (significant, p < 0.001).

days (0.0004): For each additional day the unit has been listed, the log of the expected number of reviews increases by 0.0004. exp(0.0004)≈1.0004. Each additional day listed is associated with a very small (0.04%) increase in the expected number of reviews (significant, p < 0.001).

room_type_Private room (-0.2640): Compared to an entire home/apartment (the baseline), private rooms have a lower expected number of reviews. exp(−0.2640)≈0.768. Private rooms are associated with approximately 23.2% fewer expected reviews (significant, p < 0.001).

room_type_Shared room (-1.1152): Compared to an entire home/apartment, shared rooms have a much lower expected number of reviews. exp(−1.1152)≈0.328. Shared rooms are associated with approximately 67.2% fewer expected reviews (significant, p < 0.001).

instant_bookable_t (0.4591): Listings that are instantly bookable have a higher expected number of reviews. exp(0.4591)≈1.582. Instantly bookable listings are associated with approximately 58.2% more expected reviews (significant, p < 0.001).

#### Summary:

The Poisson regression model reveals several factors associated with the number of reviews (our proxy for bookings) of Airbnb listings in New York City. Listings with more bathrooms and bedrooms tend to have more reviews. Higher prices are associated with a very slight decrease in reviews. Review scores, particularly for cleanliness, location, and value, show a strong positive association with the number of reviews. The longer a listing has been active (more days), the slightly more reviews it tends to have. Compared to entire homes/apartments, private and especially shared rooms tend to have significantly fewer reviews. Finally, listings that are instantly bookable have a substantially higher number of reviews. All these effects are statistically significant at the p < 0.001 level.

This analysis provides insights into what factors might drive the popularity (as measured by the number of reviews) of Airbnb listings in NYC.



